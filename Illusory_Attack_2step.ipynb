{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VduM1rBoB6nF",
        "outputId": "640cea48-d69d-4687-a3ee-16d2c0955164"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7e02960047f0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aNNvcyHnYdtz",
        "outputId": "33d17fca-3f05-435e-aae7-c7fa16b0bcb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.0+cu121\n",
            "Uninstalling torch-2.5.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.5.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.0.50\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.0.50:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.0.50\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.0 triton-3.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3e695ac380294ad3b5eda1610d3ade6e",
              "pip_warning": {
                "packages": [
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall torch\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq5ON0-uLHkY",
        "outputId": "89308559-365f-4260-f750-2d85895eb009"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-780f9f451008>:177: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  adversary_policy = lambda s0: f.softmax(params[s0])\n",
            "<ipython-input-5-780f9f451008>:178: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  adversary_policy1 = lambda s1, o0, a0: f.softmax(params1[s1, o0, a0])\n",
            "<ipython-input-5-780f9f451008>:226: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob1 = f.softmax(params[traj[\"obs\"]])[traj[\"action\"]]\n",
            "<ipython-input-5-780f9f451008>:227: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob2 = th.squeeze(f.softmax(params1[traj[\"obs1\"], traj[\"action\"], traj[\"victim_action\"]]))[traj[\"action1\"]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [[0.0049, 0.9951],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5939, 0.4061]],\n",
            "\n",
            "         [[0.9797, 0.0203],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4708.908392958927\n",
            "kl:  0.067640949984472\n",
            "TEST REWARD MEAN:  -0.405\n",
            "LOSS: 0.3473496217032405\n",
            "params: tensor([[0.0022, 0.9978],\n",
            "        [0.9955, 0.0045]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3316, 0.6684]],\n",
            "\n",
            "         [[0.0049, 0.9951],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5899, 0.4101]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4715.645163406483\n",
            "kl:  0.06737531617282408\n",
            "TEST REWARD MEAN:  -0.417\n",
            "LOSS: 0.3743674251773435\n",
            "params: tensor([[0.0022, 0.9978],\n",
            "        [0.9956, 0.0044]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3285, 0.6715]],\n",
            "\n",
            "         [[0.0049, 0.9951],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5956, 0.4044]],\n",
            "\n",
            "         [[0.9798, 0.0202],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4722.393549894325\n",
            "kl:  0.06745637192519487\n",
            "TEST REWARD MEAN:  -0.4985\n",
            "LOSS: 0.34472447099394316\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9956, 0.0044]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3163, 0.6837]],\n",
            "\n",
            "         [[0.0049, 0.9951],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5932, 0.4068]],\n",
            "\n",
            "         [[0.9798, 0.0202],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4729.117333330468\n",
            "kl:  0.06730529968807444\n",
            "TEST REWARD MEAN:  -0.4115\n",
            "LOSS: 0.43326065097978556\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3132, 0.6868]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5943, 0.4057]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4735.809320952513\n",
            "kl:  0.0669224520635189\n",
            "TEST REWARD MEAN:  -0.404\n",
            "LOSS: 0.3711443759617324\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3078, 0.6922]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5979, 0.4021]],\n",
            "\n",
            "         [[0.9801, 0.0199],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4742.501520047955\n",
            "kl:  0.06691940152487728\n",
            "TEST REWARD MEAN:  -0.3925\n",
            "LOSS: 0.3607105382282232\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9956, 0.0044]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2921, 0.7079]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5934, 0.4066]],\n",
            "\n",
            "         [[0.9801, 0.0199],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4749.143901256565\n",
            "kl:  0.06651629965464607\n",
            "TEST REWARD MEAN:  -0.38\n",
            "LOSS: 0.37454041942683236\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2927, 0.7073]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6018, 0.3982]],\n",
            "\n",
            "         [[0.9800, 0.0200],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4755.776625570515\n",
            "kl:  0.06626866957317193\n",
            "TEST REWARD MEAN:  -0.419\n",
            "LOSS: 0.28732238652081554\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2967, 0.7033]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6039, 0.3961]],\n",
            "\n",
            "         [[0.9800, 0.0200],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4762.4522370973145\n",
            "kl:  0.06672646409802181\n",
            "TEST REWARD MEAN:  -0.4145\n",
            "LOSS: 0.2657241842587957\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3000, 0.7000]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5987, 0.4013]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4769.1294149879295\n",
            "kl:  0.06679368206222222\n",
            "TEST REWARD MEAN:  -0.353\n",
            "LOSS: 0.2741524667078794\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3002, 0.6998]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5921, 0.4079]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4775.772889961011\n",
            "kl:  0.06647808698952684\n",
            "TEST REWARD MEAN:  -0.425\n",
            "LOSS: 0.4532462874892546\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2904, 0.7096]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5951, 0.4049]],\n",
            "\n",
            "         [[0.9801, 0.0199],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4782.387376182246\n",
            "kl:  0.06616201151844286\n",
            "TEST REWARD MEAN:  -0.3435\n",
            "LOSS: 0.3021461088350244\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2889, 0.7111]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5958, 0.4042]],\n",
            "\n",
            "         [[0.9800, 0.0200],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4788.993293006161\n",
            "kl:  0.06606067976595174\n",
            "TEST REWARD MEAN:  -0.4275\n",
            "LOSS: 0.28090675796737713\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2906, 0.7094]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5954, 0.4046]],\n",
            "\n",
            "         [[0.9800, 0.0200],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4795.600395157723\n",
            "kl:  0.0660675841184347\n",
            "TEST REWARD MEAN:  -0.3965\n",
            "LOSS: 0.34005407042035046\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2840, 0.7160]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6037, 0.3963]],\n",
            "\n",
            "         [[0.9800, 0.0200],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4802.224508416168\n",
            "kl:  0.0662128076281085\n",
            "TEST REWARD MEAN:  -0.458\n",
            "LOSS: 0.30746106711031607\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2854, 0.7146]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5985, 0.4015]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4808.845787529967\n",
            "kl:  0.06624147287949793\n",
            "TEST REWARD MEAN:  -0.3655\n",
            "LOSS: 0.30746417572384\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2833, 0.7167]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6006, 0.3994]],\n",
            "\n",
            "         [[0.9799, 0.0201],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4815.457066538347\n",
            "kl:  0.06610736872417719\n",
            "TEST REWARD MEAN:  -0.36\n",
            "LOSS: 0.3290901470597244\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2885, 0.7115]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6031, 0.3969]],\n",
            "\n",
            "         [[0.9802, 0.0198],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4822.090911019786\n",
            "kl:  0.06630147702093676\n",
            "TEST REWARD MEAN:  -0.435\n",
            "LOSS: 0.3585123573851506\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2914, 0.7086]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6064, 0.3936]],\n",
            "\n",
            "         [[0.9802, 0.0198],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4828.760351088848\n",
            "kl:  0.06665997202347748\n",
            "TEST REWARD MEAN:  -0.4025\n",
            "LOSS: 0.28483610880029975\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2978, 0.7022]],\n",
            "\n",
            "         [[0.0048, 0.9952],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6069, 0.3931]],\n",
            "\n",
            "         [[0.9802, 0.0198],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4835.461552286301\n",
            "kl:  0.06698239630962459\n",
            "TEST REWARD MEAN:  -0.363\n",
            "LOSS: 0.388386161595608\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.2998, 0.7002]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6012, 0.3988]],\n",
            "\n",
            "         [[0.9801, 0.0199],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4842.159626153649\n",
            "kl:  0.06701116591272085\n",
            "TEST REWARD MEAN:  -0.4765\n",
            "LOSS: 0.28278848825221026\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3099, 0.6901]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6010, 0.3990]],\n",
            "\n",
            "         [[0.9801, 0.0199],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4848.865595375062\n",
            "kl:  0.06701971208204424\n",
            "TEST REWARD MEAN:  -0.4425\n",
            "LOSS: 0.34544470776169905\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3125, 0.6875]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6017, 0.3983]],\n",
            "\n",
            "         [[0.9803, 0.0197],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4855.59931333666\n",
            "kl:  0.06731992062212395\n",
            "TEST REWARD MEAN:  -0.3895\n",
            "LOSS: 0.32371210005656625\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3160, 0.6840]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5994, 0.4006]],\n",
            "\n",
            "         [[0.9806, 0.0194],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4862.340208429464\n",
            "kl:  0.06741019568219614\n",
            "TEST REWARD MEAN:  -0.3995\n",
            "LOSS: 0.31483355800525914\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3195, 0.6805]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5938, 0.4062]],\n",
            "\n",
            "         [[0.9808, 0.0192],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4869.0670394800845\n",
            "kl:  0.06729298540083409\n",
            "TEST REWARD MEAN:  -0.3825\n",
            "LOSS: 0.3897428921842047\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3151, 0.6849]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5956, 0.4044]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4875.7800720132\n",
            "kl:  0.06713508270824836\n",
            "TEST REWARD MEAN:  -0.366\n",
            "LOSS: 0.30582248971165243\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3207, 0.6793]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5944, 0.4056]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4882.499021965882\n",
            "kl:  0.06717465399372241\n",
            "TEST REWARD MEAN:  -0.376\n",
            "LOSS: 0.27499321928852294\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3289, 0.6711]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5976, 0.4024]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4889.255264962204\n",
            "kl:  0.06750618626550423\n",
            "TEST REWARD MEAN:  -0.4165\n",
            "LOSS: 0.2644539325143675\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3271, 0.6729]],\n",
            "\n",
            "         [[0.0047, 0.9953],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5953, 0.4047]],\n",
            "\n",
            "         [[0.9806, 0.0194],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4896.024241073529\n",
            "kl:  0.06771350623134297\n",
            "TEST REWARD MEAN:  -0.3945\n",
            "LOSS: 0.3691887346149113\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3327, 0.6673]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5955, 0.4045]],\n",
            "\n",
            "         [[0.9806, 0.0194],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4902.796167562183\n",
            "kl:  0.06769401315269749\n",
            "TEST REWARD MEAN:  -0.419\n",
            "LOSS: 0.3980111281106801\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3305, 0.6695]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6021, 0.3979]],\n",
            "\n",
            "         [[0.9808, 0.0192],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4909.600169802016\n",
            "kl:  0.06800231078209155\n",
            "TEST REWARD MEAN:  -0.4085\n",
            "LOSS: 0.27767104420167604\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3303, 0.6697]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5957, 0.4043]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4916.3954601954265\n",
            "kl:  0.06800024705463349\n",
            "TEST REWARD MEAN:  -0.4045\n",
            "LOSS: 0.3109651278463611\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3324, 0.6676]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5964, 0.4036]],\n",
            "\n",
            "         [[0.9809, 0.0191],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4923.177141711832\n",
            "kl:  0.0678024469235352\n",
            "TEST REWARD MEAN:  -0.442\n",
            "LOSS: 0.32916902137459525\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3369, 0.6631]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5933, 0.4067]],\n",
            "\n",
            "         [[0.9808, 0.0192],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4929.963518180939\n",
            "kl:  0.06786704872548519\n",
            "TEST REWARD MEAN:  -0.4235\n",
            "LOSS: 0.3752013911165838\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9957, 0.0043]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3416, 0.6584]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5890, 0.4110]],\n",
            "\n",
            "         [[0.9810, 0.0190],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4936.742472002395\n",
            "kl:  0.06780051525739515\n",
            "TEST REWARD MEAN:  -0.4095\n",
            "LOSS: 0.3614133613567573\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3443, 0.6557]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5859, 0.4141]],\n",
            "\n",
            "         [[0.9810, 0.0190],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4943.511573984934\n",
            "kl:  0.06769987601247199\n",
            "TEST REWARD MEAN:  -0.416\n",
            "LOSS: 0.30804717379863344\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3427, 0.6573]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5885, 0.4115]],\n",
            "\n",
            "         [[0.9809, 0.0191],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4950.283087033205\n",
            "kl:  0.0677033665979151\n",
            "TEST REWARD MEAN:  -0.39\n",
            "LOSS: 0.30053838915000686\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3417, 0.6583]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5893, 0.4107]],\n",
            "\n",
            "         [[0.9809, 0.0191],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4957.060531255011\n",
            "kl:  0.06777338534207916\n",
            "TEST REWARD MEAN:  -0.3935\n",
            "LOSS: 0.30506141483989874\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3354, 0.6646]],\n",
            "\n",
            "         [[0.0046, 0.9954],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5828, 0.4172]],\n",
            "\n",
            "         [[0.9809, 0.0191],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4963.799002678405\n",
            "kl:  0.06745657071579944\n",
            "TEST REWARD MEAN:  -0.4155\n",
            "LOSS: 0.3783336759788399\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3236, 0.6764]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5842, 0.4158]],\n",
            "\n",
            "         [[0.9808, 0.0192],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4970.4837791467735\n",
            "kl:  0.0668863910345141\n",
            "TEST REWARD MEAN:  -0.4095\n",
            "LOSS: 0.40172768637021905\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3202, 0.6798]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5853, 0.4147]],\n",
            "\n",
            "         [[0.9808, 0.0192],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4977.1481730504365\n",
            "kl:  0.06664948275598286\n",
            "TEST REWARD MEAN:  -0.4055\n",
            "LOSS: 0.31109446031922683\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3255, 0.6745]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5809, 0.4191]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4983.80577670816\n",
            "kl:  0.06658387012241518\n",
            "TEST REWARD MEAN:  -0.352\n",
            "LOSS: 0.30510304551967876\n",
            "params: tensor([[0.0021, 0.9979],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3243, 0.6757]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5818, 0.4182]],\n",
            "\n",
            "         [[0.9807, 0.0193],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4990.460479390784\n",
            "kl:  0.06654583798668545\n",
            "TEST REWARD MEAN:  -0.398\n",
            "LOSS: 0.34975931916006914\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3253, 0.6747]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5802, 0.4198]],\n",
            "\n",
            "         [[0.9810, 0.0190],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  4997.112278862181\n",
            "kl:  0.0665243049115165\n",
            "TEST REWARD MEAN:  -0.393\n",
            "LOSS: 0.3396129919782667\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3301, 0.6699]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5829, 0.4171]],\n",
            "\n",
            "         [[0.9812, 0.0188],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5003.781458355598\n",
            "kl:  0.06665454890388534\n",
            "TEST REWARD MEAN:  -0.338\n",
            "LOSS: 0.29948504311465524\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3314, 0.6686]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5744, 0.4256]],\n",
            "\n",
            "         [[0.9812, 0.0188],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5010.439368966212\n",
            "kl:  0.06663079424542968\n",
            "TEST REWARD MEAN:  -0.4285\n",
            "LOSS: 0.39247658224499665\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3320, 0.6680]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5715, 0.4285]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5017.065101735177\n",
            "kl:  0.06627333646959721\n",
            "TEST REWARD MEAN:  -0.4025\n",
            "LOSS: 0.3211379975483281\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3372, 0.6628]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5738, 0.4262]],\n",
            "\n",
            "         [[0.9816, 0.0184],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5023.702984627585\n",
            "kl:  0.06634314765219142\n",
            "TEST REWARD MEAN:  -0.408\n",
            "LOSS: 0.3458604368905483\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3314, 0.6686]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5819, 0.4181]],\n",
            "\n",
            "         [[0.9816, 0.0184],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5030.373879152976\n",
            "kl:  0.06667827357523215\n",
            "TEST REWARD MEAN:  -0.4175\n",
            "LOSS: 0.3009167489773467\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3313, 0.6687]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5844, 0.4156]],\n",
            "\n",
            "         [[0.9816, 0.0184],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5037.067614726829\n",
            "kl:  0.06692103246518451\n",
            "TEST REWARD MEAN:  -0.3845\n",
            "LOSS: 0.26967129865111417\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3305, 0.6695]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5820, 0.4180]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5043.757896232003\n",
            "kl:  0.06692272773761974\n",
            "TEST REWARD MEAN:  -0.3855\n",
            "LOSS: 0.32287414893220456\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3363, 0.6637]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5819, 0.4181]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5050.452275735165\n",
            "kl:  0.06691964364152582\n",
            "TEST REWARD MEAN:  -0.3655\n",
            "LOSS: 0.30736861878413413\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3380, 0.6620]],\n",
            "\n",
            "         [[0.0045, 0.9955],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5873, 0.4127]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5057.181545250194\n",
            "kl:  0.0672487901053529\n",
            "TEST REWARD MEAN:  -0.424\n",
            "LOSS: 0.516647857756912\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3285, 0.6715]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5947, 0.4053]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5063.937426535123\n",
            "kl:  0.06754687803659505\n",
            "TEST REWARD MEAN:  -0.3565\n",
            "LOSS: 0.2818627807169514\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3372, 0.6628]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5864, 0.4136]],\n",
            "\n",
            "         [[0.9815, 0.0185],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5070.686560902682\n",
            "kl:  0.0675133462284553\n",
            "TEST REWARD MEAN:  -0.4265\n",
            "LOSS: 0.4413226582557056\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3352, 0.6648]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5838, 0.4162]],\n",
            "\n",
            "         [[0.9819, 0.0181],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5077.412064495858\n",
            "kl:  0.06728014031578415\n",
            "TEST REWARD MEAN:  -0.5125\n",
            "LOSS: 0.3335017517789498\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9958, 0.0042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3352, 0.6648]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5884, 0.4116]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5084.143986481181\n",
            "kl:  0.0672865085493566\n",
            "TEST REWARD MEAN:  -0.431\n",
            "LOSS: 0.420017475105967\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3331, 0.6669]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5836, 0.4164]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5090.867824103502\n",
            "kl:  0.06728039475703668\n",
            "TEST REWARD MEAN:  -0.352\n",
            "LOSS: 0.34856950103327405\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3361, 0.6639]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5866, 0.4134]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5097.59102713992\n",
            "kl:  0.06719826284517244\n",
            "TEST REWARD MEAN:  -0.428\n",
            "LOSS: 0.32653208296243164\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3250, 0.6750]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5855, 0.4145]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5104.30048518748\n",
            "kl:  0.06714765794129496\n",
            "TEST REWARD MEAN:  -0.4535\n",
            "LOSS: 0.3482805657863382\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3268, 0.6732]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5850, 0.4150]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5110.9878354617795\n",
            "kl:  0.06687013286068692\n",
            "TEST REWARD MEAN:  -0.4365\n",
            "LOSS: 0.3809636005700132\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3323, 0.6677]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5821, 0.4179]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5117.678859515192\n",
            "kl:  0.06690624650758843\n",
            "TEST REWARD MEAN:  -0.4095\n",
            "LOSS: 0.3182152004488729\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3376, 0.6624]],\n",
            "\n",
            "         [[0.0044, 0.9956],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5783, 0.4217]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5124.369174223336\n",
            "kl:  0.06690782130198111\n",
            "TEST REWARD MEAN:  -0.4155\n",
            "LOSS: 0.29228181017760785\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3463, 0.6537]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5778, 0.4222]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5131.074469941908\n",
            "kl:  0.0670214195921457\n",
            "TEST REWARD MEAN:  -0.449\n",
            "LOSS: 0.3071556618658626\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3417, 0.6583]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5735, 0.4265]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5137.7679270177505\n",
            "kl:  0.06698253283669911\n",
            "TEST REWARD MEAN:  -0.466\n",
            "LOSS: 0.32381280705014104\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3415, 0.6585]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5656, 0.4344]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5144.410613857579\n",
            "kl:  0.06647996621120988\n",
            "TEST REWARD MEAN:  -0.4135\n",
            "LOSS: 0.3259178809642814\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3450, 0.6550]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5659, 0.4341]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5151.037961280951\n",
            "kl:  0.0662575460046363\n",
            "TEST REWARD MEAN:  -0.406\n",
            "LOSS: 0.2740845225226275\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3426, 0.6574]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5626, 0.4374]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5157.655660276267\n",
            "kl:  0.06620786590602312\n",
            "TEST REWARD MEAN:  -0.432\n",
            "LOSS: 0.4126881640550459\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3296, 0.6704]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5647, 0.4353]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5164.239666428425\n",
            "kl:  0.0658760083682337\n",
            "TEST REWARD MEAN:  -0.38\n",
            "LOSS: 0.2962384478010079\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3339, 0.6661]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5629, 0.4371]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5170.809893273817\n",
            "kl:  0.06569779070146063\n",
            "TEST REWARD MEAN:  -0.4345\n",
            "LOSS: 0.2619907637499142\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3437, 0.6563]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5603, 0.4397]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5177.393593965802\n",
            "kl:  0.06581590126043674\n",
            "TEST REWARD MEAN:  -0.439\n",
            "LOSS: 0.30838578823546503\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3457, 0.6543]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5619, 0.4381]],\n",
            "\n",
            "         [[0.9820, 0.0180],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5183.996572671957\n",
            "kl:  0.0660121375854278\n",
            "TEST REWARD MEAN:  -0.4265\n",
            "LOSS: 0.395006333513447\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3432, 0.6568]],\n",
            "\n",
            "         [[0.0043, 0.9957],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5553, 0.4447]],\n",
            "\n",
            "         [[0.9823, 0.0177],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5190.579927013072\n",
            "kl:  0.0658839994494414\n",
            "TEST REWARD MEAN:  -0.3425\n",
            "LOSS: 0.42182827595720424\n",
            "params: tensor([[0.0020, 0.9980],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3421, 0.6579]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5475, 0.4525]],\n",
            "\n",
            "         [[0.9823, 0.0177],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5197.112287863745\n",
            "kl:  0.06537501950296701\n",
            "TEST REWARD MEAN:  -0.422\n",
            "LOSS: 0.2765127589641834\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3441, 0.6559]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5461, 0.4539]],\n",
            "\n",
            "         [[0.9823, 0.0177],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5203.620699731337\n",
            "kl:  0.06508547327998757\n",
            "TEST REWARD MEAN:  -0.343\n",
            "LOSS: 0.39595422808805625\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3379, 0.6621]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5503, 0.4497]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5210.130109793977\n",
            "kl:  0.06509115662454734\n",
            "TEST REWARD MEAN:  -0.3985\n",
            "LOSS: 0.3520125282062967\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3383, 0.6617]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5544, 0.4456]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5216.655043434833\n",
            "kl:  0.06522329562241228\n",
            "TEST REWARD MEAN:  -0.3975\n",
            "LOSS: 0.3411537640951802\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3355, 0.6645]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5546, 0.4454]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5223.186946782257\n",
            "kl:  0.06532797004190488\n",
            "TEST REWARD MEAN:  -0.341\n",
            "LOSS: 0.30900916141094187\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3356, 0.6644]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5540, 0.4460]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5229.712888196429\n",
            "kl:  0.0652629084097425\n",
            "TEST REWARD MEAN:  -0.459\n",
            "LOSS: 0.3055230416438623\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3343, 0.6657]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5531, 0.4469]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5236.231360621756\n",
            "kl:  0.06519548937421499\n",
            "TEST REWARD MEAN:  -0.421\n",
            "LOSS: 0.42306812345735895\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3291, 0.6709]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5649, 0.4351]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5242.774049746656\n",
            "kl:  0.06537329348296803\n",
            "TEST REWARD MEAN:  -0.3865\n",
            "LOSS: 0.34128001043525513\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3303, 0.6697]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5701, 0.4299]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5249.362209815897\n",
            "kl:  0.06584306065522932\n",
            "TEST REWARD MEAN:  -0.417\n",
            "LOSS: 0.31053873596491727\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3332, 0.6668]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5783, 0.4217]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5256.003937277784\n",
            "kl:  0.06635096220290122\n",
            "TEST REWARD MEAN:  -0.3965\n",
            "LOSS: 0.27421341466352195\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3423, 0.6577]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5806, 0.4194]],\n",
            "\n",
            "         [[0.9821, 0.0179],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5262.704407821878\n",
            "kl:  0.06695229507635128\n",
            "TEST REWARD MEAN:  -0.418\n",
            "LOSS: 0.33310568516056716\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3428, 0.6572]],\n",
            "\n",
            "         [[0.0042, 0.9958],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5835, 0.4165]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5269.440970942368\n",
            "kl:  0.06734342698678739\n",
            "TEST REWARD MEAN:  -0.4215\n",
            "LOSS: 0.41799592826722465\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9960, 0.0040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3418, 0.6582]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5866, 0.4134]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5276.197031816372\n",
            "kl:  0.06754335725719865\n",
            "TEST REWARD MEAN:  -0.4135\n",
            "LOSS: 0.4124118223645342\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3430, 0.6570]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5815, 0.4185]],\n",
            "\n",
            "         [[0.9823, 0.0177],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5282.945301422519\n",
            "kl:  0.06751128543334416\n",
            "TEST REWARD MEAN:  -0.458\n",
            "LOSS: 0.31990489561077207\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3368, 0.6632]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5787, 0.4213]],\n",
            "\n",
            "         [[0.9823, 0.0177],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5289.655738167629\n",
            "kl:  0.06714977144810834\n",
            "TEST REWARD MEAN:  -0.445\n",
            "LOSS: 0.28817941982255035\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3392, 0.6608]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5801, 0.4199]],\n",
            "\n",
            "         [[0.9822, 0.0178],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5296.3564189832405\n",
            "kl:  0.06698703681512738\n",
            "TEST REWARD MEAN:  -0.423\n",
            "LOSS: 0.3463518910217619\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3472, 0.6528]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5778, 0.4222]],\n",
            "\n",
            "         [[0.9824, 0.0176],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5303.075290039501\n",
            "kl:  0.06717175335418371\n",
            "TEST REWARD MEAN:  -0.398\n",
            "LOSS: 0.3026630265976179\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3487, 0.6513]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5799, 0.4201]],\n",
            "\n",
            "         [[0.9824, 0.0176],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5309.8132878491\n",
            "kl:  0.0673591194754693\n",
            "TEST REWARD MEAN:  -0.4285\n",
            "LOSS: 0.34184417436597553\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3455, 0.6545]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5749, 0.4251]],\n",
            "\n",
            "         [[0.9825, 0.0175],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5316.5350008248715\n",
            "kl:  0.06726414545554076\n",
            "TEST REWARD MEAN:  -0.4465\n",
            "LOSS: 0.27971543959867956\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3470, 0.6530]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5727, 0.4273]],\n",
            "\n",
            "         [[0.9825, 0.0175],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5323.230697221137\n",
            "kl:  0.06696570982487855\n",
            "TEST REWARD MEAN:  -0.4575\n",
            "LOSS: 0.35984408670744567\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3462, 0.6538]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5709, 0.4291]],\n",
            "\n",
            "         [[0.9824, 0.0176],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5329.9140685038155\n",
            "kl:  0.0668490397012521\n",
            "TEST REWARD MEAN:  -0.432\n",
            "LOSS: 0.35200212264154196\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3439, 0.6561]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5701, 0.4299]],\n",
            "\n",
            "         [[0.9826, 0.0174],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5336.582819617178\n",
            "kl:  0.06670153284773173\n",
            "TEST REWARD MEAN:  -0.394\n",
            "LOSS: 0.446244313245765\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3524, 0.6476]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5747, 0.4253]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5343.281202263892\n",
            "kl:  0.06691740793880778\n",
            "TEST REWARD MEAN:  -0.4215\n",
            "LOSS: 0.3052448358696135\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3510, 0.6490]],\n",
            "\n",
            "         [[0.0041, 0.9959],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5764, 0.4236]],\n",
            "\n",
            "         [[0.9828, 0.0172],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5350.013749713776\n",
            "kl:  0.06731864469003128\n",
            "TEST REWARD MEAN:  -0.4255\n",
            "LOSS: 0.34728081751275475\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3505, 0.6495]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5822, 0.4178]],\n",
            "\n",
            "         [[0.9828, 0.0172],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5356.770396186218\n",
            "kl:  0.06752792180739703\n",
            "TEST REWARD MEAN:  -0.4085\n",
            "LOSS: 0.26950071430436395\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3489, 0.6511]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5800, 0.4200]],\n",
            "\n",
            "         [[0.9828, 0.0172],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5363.532278509208\n",
            "kl:  0.06764131245989975\n",
            "TEST REWARD MEAN:  -0.3815\n",
            "LOSS: 0.3246960608541839\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3484, 0.6516]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5840, 0.4160]],\n",
            "\n",
            "         [[0.9828, 0.0172],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5370.298463948371\n",
            "kl:  0.06763529485560843\n",
            "TEST REWARD MEAN:  -0.3915\n",
            "LOSS: 0.3338856588323245\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3540, 0.6460]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5755, 0.4245]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5377.056881779085\n",
            "kl:  0.06762040789426703\n",
            "TEST REWARD MEAN:  -0.4105\n",
            "LOSS: 0.32199101996517887\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3657, 0.6343]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5759, 0.4241]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5383.827048527115\n",
            "kl:  0.06764963037215961\n",
            "TEST REWARD MEAN:  -0.3665\n",
            "LOSS: 0.31543132412379865\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3726, 0.6274]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5741, 0.4259]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5390.6295908638285\n",
            "kl:  0.06800978684013476\n",
            "TEST REWARD MEAN:  -0.3865\n",
            "LOSS: 0.2536100592085384\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3817, 0.6183]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5728, 0.4272]],\n",
            "\n",
            "         [[0.9828, 0.0172],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5397.45556563626\n",
            "kl:  0.06822980459071971\n",
            "TEST REWARD MEAN:  -0.4165\n",
            "LOSS: 0.3905614501262663\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3779, 0.6221]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5741, 0.4259]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5404.291600954528\n",
            "kl:  0.06836698919346415\n",
            "TEST REWARD MEAN:  -0.3825\n",
            "LOSS: 0.3619041655893006\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3724, 0.6276]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5737, 0.4263]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5411.110023743376\n",
            "kl:  0.06821093891743715\n",
            "TEST REWARD MEAN:  -0.4015\n",
            "LOSS: 0.28094765526477694\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3857, 0.6143]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5789, 0.4211]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5417.968836749274\n",
            "kl:  0.06849107773562413\n",
            "TEST REWARD MEAN:  -0.463\n",
            "LOSS: 0.3752837202868878\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3890, 0.6110]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5789, 0.4211]],\n",
            "\n",
            "         [[0.9831, 0.0169],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5424.880239311088\n",
            "kl:  0.0690991845108852\n",
            "TEST REWARD MEAN:  -0.4495\n",
            "LOSS: 0.32589089569017965\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3887, 0.6113]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5769, 0.4231]],\n",
            "\n",
            "         [[0.9831, 0.0169],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5431.789083182605\n",
            "kl:  0.06910553552458616\n",
            "TEST REWARD MEAN:  -0.3835\n",
            "LOSS: 0.389096447167131\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3860, 0.6140]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5770, 0.4230]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5438.683723541648\n",
            "kl:  0.06895822533637733\n",
            "TEST REWARD MEAN:  -0.4305\n",
            "LOSS: 0.4001718592464053\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3887, 0.6113]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5764, 0.4236]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5445.578054554805\n",
            "kl:  0.0689340434375409\n",
            "TEST REWARD MEAN:  -0.3825\n",
            "LOSS: 0.33230382738909564\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3968, 0.6032]],\n",
            "\n",
            "         [[0.0040, 0.9960],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5757, 0.4243]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5452.494159895355\n",
            "kl:  0.06912872059485826\n",
            "TEST REWARD MEAN:  -0.3825\n",
            "LOSS: 0.34665736740282155\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3971, 0.6029]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5753, 0.4247]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5459.423627813898\n",
            "kl:  0.06929723534459156\n",
            "TEST REWARD MEAN:  -0.431\n",
            "LOSS: 0.3878895396560851\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3993, 0.6007]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5753, 0.4247]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5466.357923307862\n",
            "kl:  0.06933195171403381\n",
            "TEST REWARD MEAN:  -0.465\n",
            "LOSS: 0.3379132589230665\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4081, 0.5919]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5788, 0.4212]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5473.334362206416\n",
            "kl:  0.06969589902711557\n",
            "TEST REWARD MEAN:  -0.4185\n",
            "LOSS: 0.280187460251462\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4065, 0.5935]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5796, 0.4204]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5480.341304599933\n",
            "kl:  0.07007098350231587\n",
            "TEST REWARD MEAN:  -0.4155\n",
            "LOSS: 0.3727010901959067\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3962, 0.6038]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5752, 0.4248]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5487.302757689692\n",
            "kl:  0.06969620427564692\n",
            "TEST REWARD MEAN:  -0.3375\n",
            "LOSS: 0.3005945289057228\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3969, 0.6031]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5795, 0.4205]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5494.247067014924\n",
            "kl:  0.06940682708339295\n",
            "TEST REWARD MEAN:  -0.401\n",
            "LOSS: 0.27844994514625065\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3953, 0.6047]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5738, 0.4262]],\n",
            "\n",
            "         [[0.9830, 0.0170],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5501.179665688465\n",
            "kl:  0.06937756078586868\n",
            "TEST REWARD MEAN:  -0.3715\n",
            "LOSS: 0.29311653755238815\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4009, 0.5991]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5709, 0.4291]],\n",
            "\n",
            "         [[0.9829, 0.0171],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5508.090387609442\n",
            "kl:  0.06910439200640872\n",
            "TEST REWARD MEAN:  -0.437\n",
            "LOSS: 0.3324479875122038\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4077, 0.5923]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5667, 0.4333]],\n",
            "\n",
            "         [[0.9832, 0.0168],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5515.0025359650235\n",
            "kl:  0.06912142871109662\n",
            "TEST REWARD MEAN:  -0.4705\n",
            "LOSS: 0.3736165349570767\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3964, 0.6036]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5634, 0.4366]],\n",
            "\n",
            "         [[0.9832, 0.0168],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5521.87364323329\n",
            "kl:  0.06878595005242519\n",
            "TEST REWARD MEAN:  -0.4505\n",
            "LOSS: 0.34092678570518964\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4040, 0.5960]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5664, 0.4336]],\n",
            "\n",
            "         [[0.9832, 0.0168],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5528.741030117288\n",
            "kl:  0.06861830913357539\n",
            "TEST REWARD MEAN:  -0.3715\n",
            "LOSS: 0.4061383237449453\n",
            "params: tensor([[0.0019, 0.9981],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3876, 0.6124]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5656, 0.4344]],\n",
            "\n",
            "         [[0.9832, 0.0168],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5535.591283226184\n",
            "kl:  0.06858035263636543\n",
            "TEST REWARD MEAN:  -0.3775\n",
            "LOSS: 0.3899652689821618\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3904, 0.6096]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5710, 0.4290]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5542.4341325390205\n",
            "kl:  0.06837729010662309\n",
            "TEST REWARD MEAN:  -0.4625\n",
            "LOSS: 0.36279578967872955\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3867, 0.6133]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5770, 0.4230]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5549.315732393522\n",
            "kl:  0.06878783131470524\n",
            "TEST REWARD MEAN:  -0.389\n",
            "LOSS: 0.3534118071827266\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3869, 0.6131]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5743, 0.4257]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5556.198986334113\n",
            "kl:  0.06885286358238629\n",
            "TEST REWARD MEAN:  -0.3775\n",
            "LOSS: 0.2858004102895623\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3971, 0.6029]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5759, 0.4241]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5563.104931667282\n",
            "kl:  0.06900061354769986\n",
            "TEST REWARD MEAN:  -0.3755\n",
            "LOSS: 0.3234759347703729\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3984, 0.6016]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5780, 0.4220]],\n",
            "\n",
            "         [[0.9832, 0.0168],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5570.049490761841\n",
            "kl:  0.06942415901220644\n",
            "TEST REWARD MEAN:  -0.421\n",
            "LOSS: 0.35794791154209965\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3983, 0.6017]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5815, 0.4185]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5577.018520750166\n",
            "kl:  0.06966331231349926\n",
            "TEST REWARD MEAN:  -0.3935\n",
            "LOSS: 0.34577014823354985\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3982, 0.6018]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5862, 0.4138]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5584.019681662528\n",
            "kl:  0.0699751809618948\n",
            "TEST REWARD MEAN:  -0.3655\n",
            "LOSS: 0.3132098968099007\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3922, 0.6078]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5842, 0.4158]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5591.013271402554\n",
            "kl:  0.06998004368445485\n",
            "TEST REWARD MEAN:  -0.449\n",
            "LOSS: 0.32853619734480305\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3858, 0.6142]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5855, 0.4145]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5597.976042079503\n",
            "kl:  0.06964756780098813\n",
            "TEST REWARD MEAN:  -0.359\n",
            "LOSS: 0.3250972538195047\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3966, 0.6034]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5868, 0.4132]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5604.962556818236\n",
            "kl:  0.06980466421948715\n",
            "TEST REWARD MEAN:  -0.374\n",
            "LOSS: 0.3185961802014714\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4017, 0.5983]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5852, 0.4148]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5611.98274206382\n",
            "kl:  0.07019083051732833\n",
            "TEST REWARD MEAN:  -0.3905\n",
            "LOSS: 0.39070374037137534\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9961, 0.0039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3964, 0.6036]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5903, 0.4097]],\n",
            "\n",
            "         [[0.9836, 0.0164],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5619.016740503579\n",
            "kl:  0.07032392435783268\n",
            "TEST REWARD MEAN:  -0.403\n",
            "LOSS: 0.3299734113862705\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3982, 0.6018]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5874, 0.4126]],\n",
            "\n",
            "         [[0.9835, 0.0165],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5626.050240729307\n",
            "kl:  0.07034927047027952\n",
            "TEST REWARD MEAN:  -0.411\n",
            "LOSS: 0.26559306271681266\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4039, 0.5961]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5867, 0.4133]],\n",
            "\n",
            "         [[0.9835, 0.0165],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5633.089413391722\n",
            "kl:  0.07036942462801582\n",
            "TEST REWARD MEAN:  -0.4205\n",
            "LOSS: 0.3246894285746535\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4051, 0.5949]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5901, 0.4099]],\n",
            "\n",
            "         [[0.9835, 0.0165],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5640.156848980114\n",
            "kl:  0.07064116100955971\n",
            "TEST REWARD MEAN:  -0.3715\n",
            "LOSS: 0.305506969682393\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4054, 0.5946]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5919, 0.4081]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5647.248153718756\n",
            "kl:  0.07089692646954095\n",
            "TEST REWARD MEAN:  -0.4525\n",
            "LOSS: 0.4622608303154375\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.4032, 0.5968]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5951, 0.4049]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5654.355662196931\n",
            "kl:  0.07105882147560698\n",
            "TEST REWARD MEAN:  -0.2955\n",
            "LOSS: 0.33214788697458675\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3976, 0.6024]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5975, 0.4025]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5661.466095441568\n",
            "kl:  0.07111252285759727\n",
            "TEST REWARD MEAN:  -0.3965\n",
            "LOSS: 0.2784170187637076\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3897, 0.6103]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5991, 0.4009]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5668.558835244215\n",
            "kl:  0.07095295704658654\n",
            "TEST REWARD MEAN:  -0.4045\n",
            "LOSS: 0.3474104311512162\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9962, 0.0038]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3760, 0.6240]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.6005, 0.3995]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5675.610003040203\n",
            "kl:  0.07056648856215247\n",
            "TEST REWARD MEAN:  -0.3815\n",
            "LOSS: 0.3549176788818923\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3784, 0.6216]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5998, 0.4002]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5682.640016321445\n",
            "kl:  0.07029341058377087\n",
            "TEST REWARD MEAN:  -0.3905\n",
            "LOSS: 0.34602410345344\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3744, 0.6256]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5976, 0.4024]],\n",
            "\n",
            "         [[0.9833, 0.0167],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5689.653651887131\n",
            "kl:  0.07017187806794659\n",
            "TEST REWARD MEAN:  -0.354\n",
            "LOSS: 0.3868885115962265\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3801, 0.6199]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5999, 0.4001]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5696.676219853673\n",
            "kl:  0.07017968230935714\n",
            "TEST REWARD MEAN:  -0.3865\n",
            "LOSS: 0.401926718033312\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3700, 0.6300]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5975, 0.4025]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5703.682973893795\n",
            "kl:  0.07013471349698344\n",
            "TEST REWARD MEAN:  -0.4045\n",
            "LOSS: 0.29878037844679173\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3594, 0.6406]],\n",
            "\n",
            "         [[0.0039, 0.9961],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5947, 0.4053]],\n",
            "\n",
            "         [[0.9834, 0.0166],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5710.621002136768\n",
            "kl:  0.06945011983809102\n",
            "TEST REWARD MEAN:  -0.349\n",
            "LOSS: 0.32750884322736995\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3679, 0.6321]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5944, 0.4056]],\n",
            "\n",
            "         [[0.9835, 0.0165],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5717.547396731732\n",
            "kl:  0.06922738962236392\n",
            "TEST REWARD MEAN:  -0.421\n",
            "LOSS: 0.38682654297977775\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3656, 0.6344]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5875, 0.4125]],\n",
            "\n",
            "         [[0.9837, 0.0163],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5724.4561677400125\n",
            "kl:  0.06915041647482624\n",
            "TEST REWARD MEAN:  -0.3585\n",
            "LOSS: 0.29545333943312235\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3733, 0.6267]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5832, 0.4168]],\n",
            "\n",
            "         [[0.9836, 0.0164],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5731.337586589454\n",
            "kl:  0.06881217862841925\n",
            "TEST REWARD MEAN:  -0.3655\n",
            "LOSS: 0.3498640916704476\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3732, 0.6268]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5869, 0.4131]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5738.235274923527\n",
            "kl:  0.06894881785074733\n",
            "TEST REWARD MEAN:  -0.414\n",
            "LOSS: 0.32542215475493635\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3678, 0.6322]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5895, 0.4105]],\n",
            "\n",
            "         [[0.9837, 0.0163],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5745.142679115619\n",
            "kl:  0.06907965930665276\n",
            "TEST REWARD MEAN:  -0.4455\n",
            "LOSS: 0.29176379400370617\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3665, 0.6335]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5914, 0.4086]],\n",
            "\n",
            "         [[0.9837, 0.0163],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5752.052255243995\n",
            "kl:  0.06908709285430395\n",
            "TEST REWARD MEAN:  -0.342\n",
            "LOSS: 0.2925080659349946\n",
            "params: tensor([[0.0018, 0.9982],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3662, 0.6338]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5950, 0.4050]],\n",
            "\n",
            "         [[0.9837, 0.0163],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5758.979845239253\n",
            "kl:  0.06925001075656863\n",
            "TEST REWARD MEAN:  -0.424\n",
            "LOSS: 0.4515429786344892\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3669, 0.6331]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5922, 0.4078]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5765.90956251745\n",
            "kl:  0.06931492794542138\n",
            "TEST REWARD MEAN:  -0.376\n",
            "LOSS: 0.3554177870508637\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3603, 0.6397]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5850, 0.4150]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5772.785294778922\n",
            "kl:  0.06884094892644432\n",
            "TEST REWARD MEAN:  -0.406\n",
            "LOSS: 0.3006520399368773\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3555, 0.6445]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5805, 0.4195]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5779.593943892959\n",
            "kl:  0.0681400929105796\n",
            "TEST REWARD MEAN:  -0.4725\n",
            "LOSS: 0.36935763147699147\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3468, 0.6532]],\n",
            "\n",
            "         [[0.0038, 0.9962],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5833, 0.4167]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5786.3696494740525\n",
            "kl:  0.06777301344319477\n",
            "TEST REWARD MEAN:  -0.406\n",
            "LOSS: 0.3779036806961874\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3443, 0.6557]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5824, 0.4176]],\n",
            "\n",
            "         [[0.9838, 0.0162],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5793.129143380444\n",
            "kl:  0.06761132605982685\n",
            "TEST REWARD MEAN:  -0.442\n",
            "LOSS: 0.3531589295752718\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9963, 0.0037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3428, 0.6572]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5793, 0.4207]],\n",
            "\n",
            "         [[0.9837, 0.0163],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5799.8659186265395\n",
            "kl:  0.06739573953208773\n",
            "TEST REWARD MEAN:  -0.3505\n",
            "LOSS: 0.49230968516616497\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9964, 0.0036]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3350, 0.6650]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5848, 0.4152]],\n",
            "\n",
            "         [[0.9839, 0.0161],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5806.594270511662\n",
            "kl:  0.06727569412903925\n",
            "TEST REWARD MEAN:  -0.378\n",
            "LOSS: 0.4496095976862756\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9964, 0.0036]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3260, 0.6740]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5924, 0.4076]],\n",
            "\n",
            "         [[0.9839, 0.0161],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5813.334954840567\n",
            "kl:  0.067390835728369\n",
            "TEST REWARD MEAN:  -0.4135\n",
            "LOSS: 0.4109832130076406\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3294, 0.6706]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5927, 0.4073]],\n",
            "\n",
            "         [[0.9841, 0.0159],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5820.092459227633\n",
            "kl:  0.06755745991825467\n",
            "TEST REWARD MEAN:  -0.39\n",
            "LOSS: 0.3704027617449562\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3276, 0.6724]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5928, 0.4072]],\n",
            "\n",
            "         [[0.9843, 0.0157],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5826.854403248864\n",
            "kl:  0.06762596407812606\n",
            "TEST REWARD MEAN:  -0.391\n",
            "LOSS: 0.3851922825440105\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3281, 0.6719]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5960, 0.4040]],\n",
            "\n",
            "         [[0.9843, 0.0157],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5833.627149305014\n",
            "kl:  0.0677021705202099\n",
            "TEST REWARD MEAN:  -0.36\n",
            "LOSS: 0.2868614336468141\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3271, 0.6729]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5902, 0.4098]],\n",
            "\n",
            "         [[0.9842, 0.0158],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5840.386607699163\n",
            "kl:  0.06763991006260528\n",
            "TEST REWARD MEAN:  -0.367\n",
            "LOSS: 0.41746765674104086\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3312, 0.6688]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5865, 0.4135]],\n",
            "\n",
            "         [[0.9844, 0.0156],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5847.122033862484\n",
            "kl:  0.06736068868082598\n",
            "TEST REWARD MEAN:  -0.426\n",
            "LOSS: 0.2975734356213035\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3341, 0.6659]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5899, 0.4101]],\n",
            "\n",
            "         [[0.9843, 0.0157],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5853.873510799362\n",
            "kl:  0.06747993161227901\n",
            "TEST REWARD MEAN:  -0.35\n",
            "LOSS: 0.270100734563902\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9965, 0.0035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3371, 0.6629]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5895, 0.4105]],\n",
            "\n",
            "         [[0.9843, 0.0157],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5860.646432569212\n",
            "kl:  0.06771901500497049\n",
            "TEST REWARD MEAN:  -0.3795\n",
            "LOSS: 0.4191807129730439\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3328, 0.6672]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5932, 0.4068]],\n",
            "\n",
            "         [[0.9844, 0.0156],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5867.42869608969\n",
            "kl:  0.06781404551776624\n",
            "TEST REWARD MEAN:  -0.319\n",
            "LOSS: 0.28091415310941814\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3358, 0.6642]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5933, 0.4067]],\n",
            "\n",
            "         [[0.9844, 0.0156],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5874.2222971534875\n",
            "kl:  0.06792231826053356\n",
            "TEST REWARD MEAN:  -0.372\n",
            "LOSS: 0.3479331328473962\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3367, 0.6633]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5925, 0.4075]],\n",
            "\n",
            "         [[0.9844, 0.0156],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5881.020906452243\n",
            "kl:  0.06798833659023867\n",
            "TEST REWARD MEAN:  -0.383\n",
            "LOSS: 0.3702284645822576\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3339, 0.6661]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5934, 0.4066]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5887.815363286107\n",
            "kl:  0.06795033308594926\n",
            "TEST REWARD MEAN:  -0.435\n",
            "LOSS: 0.40024072623128437\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3324, 0.6676]],\n",
            "\n",
            "         [[0.0037, 0.9963],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5879, 0.4121]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5894.582905575942\n",
            "kl:  0.06771978631817731\n",
            "TEST REWARD MEAN:  -0.4\n",
            "LOSS: 0.3149858146212162\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3271, 0.6729]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5914, 0.4086]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5901.331514920784\n",
            "kl:  0.06748410987753127\n",
            "TEST REWARD MEAN:  -0.4295\n",
            "LOSS: 0.39080768527364146\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3275, 0.6725]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5899, 0.4101]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5908.076831381202\n",
            "kl:  0.06746074864194071\n",
            "TEST REWARD MEAN:  -0.454\n",
            "LOSS: 0.3441343291787572\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3316, 0.6684]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5880, 0.4120]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5914.82085594126\n",
            "kl:  0.06743619919083628\n",
            "TEST REWARD MEAN:  -0.404\n",
            "LOSS: 0.2811041816870817\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3323, 0.6677]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5865, 0.4135]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5921.562683440261\n",
            "kl:  0.06742576886707444\n",
            "TEST REWARD MEAN:  -0.4435\n",
            "LOSS: 0.28830852522826333\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3382, 0.6618]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5891, 0.4109]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5928.324125687848\n",
            "kl:  0.06757204344854281\n",
            "TEST REWARD MEAN:  -0.388\n",
            "LOSS: 0.290143444323394\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3385, 0.6615]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5857, 0.4143]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5935.092500466308\n",
            "kl:  0.06770649726118394\n",
            "TEST REWARD MEAN:  -0.425\n",
            "LOSS: 0.38561799559424326\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3312, 0.6688]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5799, 0.4201]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5941.812216602794\n",
            "kl:  0.06726701088155038\n",
            "TEST REWARD MEAN:  -0.419\n",
            "LOSS: 0.2720567925075949\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3304, 0.6696]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5756, 0.4244]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5948.482808029559\n",
            "kl:  0.06673809610265133\n",
            "TEST REWARD MEAN:  -0.4485\n",
            "LOSS: 0.25992378011026696\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3400, 0.6600]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5736, 0.4264]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5955.152557390625\n",
            "kl:  0.06667216140011274\n",
            "TEST REWARD MEAN:  -0.311\n",
            "LOSS: 0.3384142018777582\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3351, 0.6649]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5754, 0.4246]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5961.829414129847\n",
            "kl:  0.06677678127217998\n",
            "TEST REWARD MEAN:  -0.3515\n",
            "LOSS: 0.2769678225902734\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3413, 0.6587]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5734, 0.4266]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5968.508836618801\n",
            "kl:  0.06678262765547484\n",
            "TEST REWARD MEAN:  -0.4275\n",
            "LOSS: 0.29552258659324676\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3479, 0.6521]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5772, 0.4228]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5975.222161117221\n",
            "kl:  0.06708056416008008\n",
            "TEST REWARD MEAN:  -0.4745\n",
            "LOSS: 0.3350639633900861\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3428, 0.6572]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5710, 0.4290]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5981.924795974042\n",
            "kl:  0.06708975076399537\n",
            "TEST REWARD MEAN:  -0.366\n",
            "LOSS: 0.29500755093778197\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3425, 0.6575]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5747, 0.4253]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5988.611669103044\n",
            "kl:  0.06684512220955023\n",
            "TEST REWARD MEAN:  -0.373\n",
            "LOSS: 0.3703602696422958\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3401, 0.6599]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5842, 0.4158]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  5995.339449997266\n",
            "kl:  0.06722236570358517\n",
            "TEST REWARD MEAN:  -0.4075\n",
            "LOSS: 0.3203128604201626\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3429, 0.6571]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5876, 0.4124]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6002.111824560889\n",
            "kl:  0.06768824980358917\n",
            "TEST REWARD MEAN:  -0.393\n",
            "LOSS: 0.4421421139539426\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3374, 0.6626]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5791, 0.4209]],\n",
            "\n",
            "         [[0.9844, 0.0156],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6008.85569388784\n",
            "kl:  0.06752007726627135\n",
            "TEST REWARD MEAN:  -0.4115\n",
            "LOSS: 0.3375383246190203\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3353, 0.6647]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5846, 0.4154]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6015.57894842768\n",
            "kl:  0.06720301192889926\n",
            "TEST REWARD MEAN:  -0.3585\n",
            "LOSS: 0.2826041123812397\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3392, 0.6608]],\n",
            "\n",
            "         [[0.0036, 0.9964],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5813, 0.4187]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6022.311799024075\n",
            "kl:  0.06733557283830231\n",
            "TEST REWARD MEAN:  -0.383\n",
            "LOSS: 0.3827142384021921\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3450, 0.6550]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5860, 0.4140]],\n",
            "\n",
            "         [[0.9845, 0.0155],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6029.072319779753\n",
            "kl:  0.0675482954438379\n",
            "TEST REWARD MEAN:  -0.4275\n",
            "LOSS: 0.34429015940570595\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3403, 0.6597]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5808, 0.4192]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6035.828228921131\n",
            "kl:  0.06761499506834827\n",
            "TEST REWARD MEAN:  -0.3595\n",
            "LOSS: 0.36207204439683904\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3346, 0.6654]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5839, 0.4161]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6042.5580477279545\n",
            "kl:  0.06729972822402272\n",
            "TEST REWARD MEAN:  -0.354\n",
            "LOSS: 0.29695094078652867\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3400, 0.6600]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5778, 0.4222]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6049.276076270679\n",
            "kl:  0.06720058053636103\n",
            "TEST REWARD MEAN:  -0.3245\n",
            "LOSS: 0.3272240190500063\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3465, 0.6535]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5720, 0.4280]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6055.978274902863\n",
            "kl:  0.06703398752841144\n",
            "TEST REWARD MEAN:  -0.4125\n",
            "LOSS: 0.3796297647726702\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3496, 0.6504]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5715, 0.4285]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6062.6802483945185\n",
            "kl:  0.06701026161738437\n",
            "TEST REWARD MEAN:  -0.413\n",
            "LOSS: 0.2774308733179341\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3576, 0.6424]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5753, 0.4247]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6069.418225900432\n",
            "kl:  0.06732136731571203\n",
            "TEST REWARD MEAN:  -0.452\n",
            "LOSS: 0.2604285475114794\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3681, 0.6319]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5774, 0.4226]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6076.215486014455\n",
            "kl:  0.06791258830088612\n",
            "TEST REWARD MEAN:  -0.395\n",
            "LOSS: 0.42637736975472296\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3692, 0.6308]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5795, 0.4205]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6083.051998735011\n",
            "kl:  0.06834346509334448\n",
            "TEST REWARD MEAN:  -0.4675\n",
            "LOSS: 0.3460650793677982\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3603, 0.6397]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5811, 0.4189]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6089.883319670688\n",
            "kl:  0.06834069114463316\n",
            "TEST REWARD MEAN:  -0.354\n",
            "LOSS: 0.30538686500914214\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3602, 0.6398]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5708, 0.4292]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6096.661904282328\n",
            "kl:  0.0678592890701066\n",
            "TEST REWARD MEAN:  -0.438\n",
            "LOSS: 0.31903216872773627\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3580, 0.6420]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5595, 0.4405]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6103.360701546186\n",
            "kl:  0.06707290046143459\n",
            "TEST REWARD MEAN:  -0.383\n",
            "LOSS: 0.3108696126848708\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3626, 0.6374]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5586, 0.4414]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6110.027810063598\n",
            "kl:  0.06665896678657963\n",
            "TEST REWARD MEAN:  -0.416\n",
            "LOSS: 0.3062107450746088\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3587, 0.6413]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5617, 0.4383]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6116.703114779962\n",
            "kl:  0.06674814810494926\n",
            "TEST REWARD MEAN:  -0.504\n",
            "LOSS: 0.34531767841977223\n",
            "params: tensor([[0.0017, 0.9983],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3608, 0.6392]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5626, 0.4374]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6123.388800747807\n",
            "kl:  0.06684190667189276\n",
            "TEST REWARD MEAN:  -0.392\n",
            "LOSS: 0.2811575130042244\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3596, 0.6404]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5632, 0.4368]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6130.080657447065\n",
            "kl:  0.06691970846341677\n",
            "TEST REWARD MEAN:  -0.4075\n",
            "LOSS: 0.3574519893207882\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3547, 0.6453]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5607, 0.4393]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6136.752510192806\n",
            "kl:  0.06675412924658235\n",
            "TEST REWARD MEAN:  -0.455\n",
            "LOSS: 0.40420243454806337\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3573, 0.6427]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5581, 0.4419]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6143.404569784103\n",
            "kl:  0.06652737066299783\n",
            "TEST REWARD MEAN:  -0.4035\n",
            "LOSS: 0.37234887934648947\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3568, 0.6432]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5613, 0.4387]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6150.063930201749\n",
            "kl:  0.06657448642645969\n",
            "TEST REWARD MEAN:  -0.3735\n",
            "LOSS: 0.3215005329855781\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3598, 0.6402]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5575, 0.4425]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6156.724557333227\n",
            "kl:  0.06661993498824972\n",
            "TEST REWARD MEAN:  -0.402\n",
            "LOSS: 0.3718425420674301\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3493, 0.6507]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5536, 0.4464]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6163.3427931988\n",
            "kl:  0.0662482995041999\n",
            "TEST REWARD MEAN:  -0.4075\n",
            "LOSS: 0.3234933664410934\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3523, 0.6477]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5575, 0.4425]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6169.951671336104\n",
            "kl:  0.06605130022970157\n",
            "TEST REWARD MEAN:  -0.3765\n",
            "LOSS: 0.33701099114551614\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3602, 0.6398]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5584, 0.4416]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6176.598111207106\n",
            "kl:  0.06642672328353064\n",
            "TEST REWARD MEAN:  -0.407\n",
            "LOSS: 0.2679731351064534\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3600, 0.6400]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5568, 0.4432]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6183.255295569951\n",
            "kl:  0.06658364661729847\n",
            "TEST REWARD MEAN:  -0.38\n",
            "LOSS: 0.3518333524665949\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3509, 0.6491]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5511, 0.4489]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6189.86787083044\n",
            "kl:  0.06619722949677824\n",
            "TEST REWARD MEAN:  -0.4325\n",
            "LOSS: 0.27709700675093907\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3485, 0.6515]],\n",
            "\n",
            "         [[0.0035, 0.9965],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5510, 0.4490]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6196.44288980668\n",
            "kl:  0.06575994184380453\n",
            "TEST REWARD MEAN:  -0.401\n",
            "LOSS: 0.333402683433876\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3494, 0.6506]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5490, 0.4510]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6203.008249401973\n",
            "kl:  0.06566318739409029\n",
            "TEST REWARD MEAN:  -0.4125\n",
            "LOSS: 0.2829412476691856\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3447, 0.6553]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5431, 0.4569]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6209.540246602175\n",
            "kl:  0.06537284388697714\n",
            "TEST REWARD MEAN:  -0.4385\n",
            "LOSS: 0.3460844339674111\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3416, 0.6584]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5480, 0.4520]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6216.058492199231\n",
            "kl:  0.06516397722029249\n",
            "TEST REWARD MEAN:  -0.398\n",
            "LOSS: 0.3508027090771537\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3436, 0.6564]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5429, 0.4571]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6222.572597706657\n",
            "kl:  0.06516392139045729\n",
            "TEST REWARD MEAN:  -0.3705\n",
            "LOSS: 0.38865171648830776\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3381, 0.6619]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5416, 0.4584]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6229.061506364614\n",
            "kl:  0.0649162017127614\n",
            "TEST REWARD MEAN:  -0.4265\n",
            "LOSS: 0.30093207332626964\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3367, 0.6633]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5459, 0.4541]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6235.549037730607\n",
            "kl:  0.0648552837361134\n",
            "TEST REWARD MEAN:  -0.392\n",
            "LOSS: 0.322904230617329\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3377, 0.6623]],\n",
            "\n",
            "         [[0.0034, 0.9966],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5421, 0.4579]],\n",
            "\n",
            "         [[0.9846, 0.0154],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6242.035455420549\n",
            "kl:  0.06488292874543296\n",
            "TEST REWARD MEAN:  -0.4165\n",
            "LOSS: 0.4205043123098724\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3342, 0.6658]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5444, 0.4556]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6248.513709795006\n",
            "kl:  0.06478192603685556\n",
            "TEST REWARD MEAN:  -0.408\n",
            "LOSS: 0.37537409471737443\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3230, 0.6770]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5418, 0.4582]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6254.963073199805\n",
            "kl:  0.06454673436617708\n",
            "TEST REWARD MEAN:  -0.4145\n",
            "LOSS: 0.2662122763329626\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3269, 0.6731]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5375, 0.4625]],\n",
            "\n",
            "         [[0.9847, 0.0153],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6261.382449689512\n",
            "kl:  0.06420482557757785\n",
            "TEST REWARD MEAN:  -0.3765\n",
            "LOSS: 0.34285875449698217\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3318, 0.6682]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5408, 0.4592]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6267.816102044884\n",
            "kl:  0.0643010023674997\n",
            "TEST REWARD MEAN:  -0.3555\n",
            "LOSS: 0.3028478984235723\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3251, 0.6749]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5446, 0.4554]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6274.264992870232\n",
            "kl:  0.06449065125050006\n",
            "TEST REWARD MEAN:  -0.425\n",
            "LOSS: 0.27334296569631356\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9967, 0.0033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3291, 0.6709]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5474, 0.4526]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6280.72942421419\n",
            "kl:  0.06461437550651956\n",
            "TEST REWARD MEAN:  -0.353\n",
            "LOSS: 0.33034973757836\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3289, 0.6711]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5523, 0.4477]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6287.223423549901\n",
            "kl:  0.06491071641840854\n",
            "TEST REWARD MEAN:  -0.403\n",
            "LOSS: 0.26741101344240054\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3339, 0.6661]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5473, 0.4527]],\n",
            "\n",
            "         [[0.9848, 0.0152],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6293.724106045196\n",
            "kl:  0.06501901562923859\n",
            "TEST REWARD MEAN:  -0.397\n",
            "LOSS: 0.39711207109514174\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3325, 0.6675]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5546, 0.4454]],\n",
            "\n",
            "         [[0.9849, 0.0151],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6300.240564371857\n",
            "kl:  0.06512549013308595\n",
            "TEST REWARD MEAN:  -0.374\n",
            "LOSS: 0.3271597390984988\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3287, 0.6713]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5502, 0.4498]],\n",
            "\n",
            "         [[0.9850, 0.0150],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6306.753120250723\n",
            "kl:  0.06516516882108792\n",
            "TEST REWARD MEAN:  -0.4055\n",
            "LOSS: 0.3208968894415899\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3285, 0.6715]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5556, 0.4444]],\n",
            "\n",
            "         [[0.9850, 0.0150],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6313.265416353153\n",
            "kl:  0.06509058246126873\n",
            "TEST REWARD MEAN:  -0.4065\n",
            "LOSS: 0.36701562531878035\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3365, 0.6635]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5576, 0.4424]],\n",
            "\n",
            "         [[0.9851, 0.0149],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6319.815251618386\n",
            "kl:  0.065456347524411\n",
            "TEST REWARD MEAN:  -0.4005\n",
            "LOSS: 0.2783794468039076\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3396, 0.6604]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5610, 0.4390]],\n",
            "\n",
            "         [[0.9851, 0.0149],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6326.4025064278785\n",
            "kl:  0.06583905848821102\n",
            "TEST REWARD MEAN:  -0.339\n",
            "LOSS: 0.3483691928627509\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3400, 0.6600]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5573, 0.4427]],\n",
            "\n",
            "         [[0.9852, 0.0148],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6332.992977001628\n",
            "kl:  0.06592676106240059\n",
            "TEST REWARD MEAN:  -0.45\n",
            "LOSS: 0.31190171893875374\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3473, 0.6527]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5576, 0.4424]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6339.589616755401\n",
            "kl:  0.06593665103097436\n",
            "TEST REWARD MEAN:  -0.35\n",
            "LOSS: 0.3013033521954291\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3465, 0.6535]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5562, 0.4438]],\n",
            "\n",
            "         [[0.9853, 0.0147],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6346.193309752944\n",
            "kl:  0.06604888466930474\n",
            "TEST REWARD MEAN:  -0.3765\n",
            "LOSS: 0.3699758162969633\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3375, 0.6625]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5547, 0.4453]],\n",
            "\n",
            "         [[0.9853, 0.0147],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6352.767984149107\n",
            "kl:  0.06578982663099293\n",
            "TEST REWARD MEAN:  -0.424\n",
            "LOSS: 0.39379807714125464\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3370, 0.6630]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5566, 0.4434]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6359.329029949511\n",
            "kl:  0.06559973570220441\n",
            "TEST REWARD MEAN:  -0.379\n",
            "LOSS: 0.3231765932853547\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3304, 0.6696]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5603, 0.4397]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6365.89400612069\n",
            "kl:  0.06565144317183025\n",
            "TEST REWARD MEAN:  -0.34\n",
            "LOSS: 0.3027678107563997\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3386, 0.6614]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5544, 0.4456]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6372.45465038167\n",
            "kl:  0.06561285172579155\n",
            "TEST REWARD MEAN:  -0.4365\n",
            "LOSS: 0.42222890017010223\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3313, 0.6687]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5534, 0.4466]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6378.994606656656\n",
            "kl:  0.06543215940376723\n",
            "TEST REWARD MEAN:  -0.42\n",
            "LOSS: 0.37560960238776314\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3350, 0.6650]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5630, 0.4370]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6385.559405074916\n",
            "kl:  0.06557485335725585\n",
            "TEST REWARD MEAN:  -0.431\n",
            "LOSS: 0.29765931898571346\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3373, 0.6627]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5594, 0.4406]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6392.149893261094\n",
            "kl:  0.06591935207920832\n",
            "TEST REWARD MEAN:  -0.4605\n",
            "LOSS: 0.395141671478577\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3323, 0.6677]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5515, 0.4485]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6398.696946329697\n",
            "kl:  0.06553760009758872\n",
            "TEST REWARD MEAN:  -0.351\n",
            "LOSS: 0.2876840096523125\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3420, 0.6580]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5476, 0.4524]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6405.220095640314\n",
            "kl:  0.0652198001615331\n",
            "TEST REWARD MEAN:  -0.419\n",
            "LOSS: 0.2743755063300541\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3511, 0.6489]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5520, 0.4480]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6411.781324864556\n",
            "kl:  0.06555188312439424\n",
            "TEST REWARD MEAN:  -0.384\n",
            "LOSS: 0.2935706367878517\n",
            "params: tensor([[0.0016, 0.9984],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3511, 0.6489]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5527, 0.4473]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6418.3724268365195\n",
            "kl:  0.06590700794083998\n",
            "TEST REWARD MEAN:  -0.326\n",
            "LOSS: 0.3126460559655815\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3405, 0.6595]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5597, 0.4403]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6424.967837893306\n",
            "kl:  0.06594947464926063\n",
            "TEST REWARD MEAN:  -0.3235\n",
            "LOSS: 0.31825489830387454\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3424, 0.6576]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5605, 0.4395]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6431.572223157665\n",
            "kl:  0.0660313492425374\n",
            "TEST REWARD MEAN:  -0.392\n",
            "LOSS: 0.3839929442013385\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3348, 0.6652]],\n",
            "\n",
            "         [[0.0033, 0.9967],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5558, 0.4442]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6438.150624078115\n",
            "kl:  0.06584176776558183\n",
            "TEST REWARD MEAN:  -0.407\n",
            "LOSS: 0.4729497764016638\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3288, 0.6712]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5636, 0.4364]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6444.717574140795\n",
            "kl:  0.06564267920985481\n",
            "TEST REWARD MEAN:  -0.3605\n",
            "LOSS: 0.3009916723037403\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3331, 0.6669]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5675, 0.4325]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6451.318676761255\n",
            "kl:  0.06597058316628933\n",
            "TEST REWARD MEAN:  -0.3725\n",
            "LOSS: 0.2852681696871717\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3286, 0.6714]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5666, 0.4334]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6457.92563203199\n",
            "kl:  0.06609258767837249\n",
            "TEST REWARD MEAN:  -0.468\n",
            "LOSS: 0.33920127226130914\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3247, 0.6753]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5650, 0.4350]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6464.508896730633\n",
            "kl:  0.06585688675592252\n",
            "TEST REWARD MEAN:  -0.3825\n",
            "LOSS: 0.31484463038139787\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3257, 0.6743]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5617, 0.4383]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6471.072134150826\n",
            "kl:  0.0656489888396708\n",
            "TEST REWARD MEAN:  -0.4615\n",
            "LOSS: 0.2702930388924547\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3298, 0.6702]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5647, 0.4353]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6477.645865855061\n",
            "kl:  0.06570413613424524\n",
            "TEST REWARD MEAN:  -0.436\n",
            "LOSS: 0.2852708533257078\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9968, 0.0032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3326, 0.6674]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5641, 0.4359]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6484.238637110761\n",
            "kl:  0.06592053487629126\n",
            "TEST REWARD MEAN:  -0.4305\n",
            "LOSS: 0.33406508493869824\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3363, 0.6637]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5638, 0.4362]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6490.841385936232\n",
            "kl:  0.0660151817992942\n",
            "TEST REWARD MEAN:  -0.4175\n",
            "LOSS: 0.37268850434349904\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3380, 0.6620]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5637, 0.4363]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6497.453347537674\n",
            "kl:  0.06611297124125176\n",
            "TEST REWARD MEAN:  -0.389\n",
            "LOSS: 0.33748311017939153\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3353, 0.6647]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5658, 0.4342]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6504.069903269446\n",
            "kl:  0.06616270971620683\n",
            "TEST REWARD MEAN:  -0.421\n",
            "LOSS: 0.3077670426191828\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3354, 0.6646]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5644, 0.4356]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6510.682764967305\n",
            "kl:  0.06613771929626347\n",
            "TEST REWARD MEAN:  -0.405\n",
            "LOSS: 0.31911046146640465\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3266, 0.6734]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5656, 0.4344]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6517.277220528075\n",
            "kl:  0.0659705597338008\n",
            "TEST REWARD MEAN:  -0.3395\n",
            "LOSS: 0.2637167860716163\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3382, 0.6618]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5640, 0.4360]],\n",
            "\n",
            "         [[0.9856, 0.0144],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6523.878435365783\n",
            "kl:  0.06597803272526165\n",
            "TEST REWARD MEAN:  -0.4245\n",
            "LOSS: 0.3910611411005254\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3317, 0.6683]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5666, 0.4334]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6530.491293184636\n",
            "kl:  0.06613589486849548\n",
            "TEST REWARD MEAN:  -0.4365\n",
            "LOSS: 0.3772763816397754\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3234, 0.6766]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5670, 0.4330]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6537.085085991236\n",
            "kl:  0.06596667733956835\n",
            "TEST REWARD MEAN:  -0.3785\n",
            "LOSS: 0.3280589891827022\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3340, 0.6660]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5653, 0.4347]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6543.682287689059\n",
            "kl:  0.06594170578504616\n",
            "TEST REWARD MEAN:  -0.368\n",
            "LOSS: 0.33054111540721104\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9969, 0.0031]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3266, 0.6734]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5650, 0.4350]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6550.276799740732\n",
            "kl:  0.06597539026594842\n",
            "TEST REWARD MEAN:  -0.4225\n",
            "LOSS: 0.37010231268477706\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9970, 0.0030]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3261, 0.6739]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5692, 0.4308]],\n",
            "\n",
            "         [[0.9855, 0.0145],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6556.871328036646\n",
            "kl:  0.06592020080278471\n",
            "TEST REWARD MEAN:  -0.4045\n",
            "LOSS: 0.37465017388487537\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9970, 0.0030]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3241, 0.6759]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5616, 0.4384]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6563.447166750496\n",
            "kl:  0.06581321218429347\n",
            "TEST REWARD MEAN:  -0.4025\n",
            "LOSS: 0.2893198972672834\n",
            "params: tensor([[0.0015, 0.9985],\n",
            "        [0.9970, 0.0030]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "params1: tensor([[[[0.4389, 0.5611],\n",
            "          [0.3222, 0.6778]],\n",
            "\n",
            "         [[0.0032, 0.9968],\n",
            "          [0.4468, 0.5532]]],\n",
            "\n",
            "\n",
            "        [[[0.6114, 0.3886],\n",
            "          [0.5605, 0.4395]],\n",
            "\n",
            "         [[0.9854, 0.0146],\n",
            "          [0.5423, 0.4577]]]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "lambda:  6569.990807355053\n",
            "kl:  0.06545001649539332\n",
            "TEST REWARD MEAN:  -0.4295\n",
            "[0.03597093264268272, 0.03650947834044895, 0.037438707558871805, 0.03859785597814852, 0.03965300107728104, 0.03902001807546135, 0.038659220841329174, 0.03888443273052508, 0.0389175304390823, 0.04097170286608684, 0.04262347620936405, 0.042416187394527155, 0.042750728223889756, 0.04252401582947836, 0.042220329640406215, 0.042243466298226666, 0.04248263746859696, 0.04327784380552828, 0.044818340749437524, 0.04511491237866371, 0.04512765948701157, 0.04607054487436973, 0.04602663328125122, 0.04641858981431555, 0.047442225349220164, 0.04847836199471019, 0.048892025312527974, 0.04859105959038951, 0.04846111857722353, 0.04819990125041584, 0.04818486985200369, 0.04863251742065789, 0.04909905607239113, 0.04902023926014588, 0.04948041240747275, 0.05017368702673932, 0.050121765776576364, 0.05000936778901258, 0.05030084528319958, 0.050467133220263374, 0.050743676067530184, 0.051603026159849, 0.05222794775187534, 0.052425926845875236, 0.052632084235870194, 0.052974389429048536, 0.05324287885043922, 0.053835218577723935, 0.05426534949507357, 0.054505466224378, 0.05497205582062204, 0.05556737825844272, 0.055587654567270106, 0.055521207472501985, 0.05617542052735236, 0.05633002480288691, 0.0556197245488309, 0.05516653358449053, 0.05514103000856886, 0.055374958221959335, 0.05552333804142485, 0.055899620381327765, 0.056545276642601464, 0.056941712689225696, 0.05692288710489656, 0.05683469279007193, 0.056846225591131304, 0.0569964583772403, 0.05698557918345915, 0.05653689126715828, 0.056290309511934596, 0.056512481526138504, 0.05650944924791799, 0.0560279802288173, 0.055795184360268674, 0.055681157417529825, 0.05570204664743225, 0.055666846876226334, 0.0555763322300935, 0.05584032771317519, 0.056202737190599, 0.05632354938055299, 0.056152500316640715, 0.05615283771585249, 0.05625975520034641, 0.05612593175396374, 0.055996346489982585, 0.05588734504707885, 0.05605487257805274, 0.05632118430114855, 0.05650048715270949, 0.05665507191718979, 0.05685529438338661, 0.057083171140429746, 0.057174413724026506, 0.057334846346332236, 0.05747535554128798, 0.05776361159159412, 0.05770930172230504, 0.0573271064256741, 0.057542537985062675, 0.057614739151777276, 0.057242951319748304, 0.057372403398007095, 0.0573170203576714, 0.05715549391995924, 0.05733826215503447, 0.05751798179047236, 0.057438011824993085, 0.05753339700316896, 0.05755899806830281, 0.057793765153454, 0.05806352959669601, 0.05794566003880972, 0.05782844764381628, 0.058000196807313074, 0.058179481086450054, 0.058192200286282296, 0.057899030726991194, 0.05755055770741833, 0.05758396417633801, 0.057920767839393206, 0.0581402295185357, 0.05815151403609238, 0.05808645681112037, 0.05814021706714773, 0.05831159554277049, 0.05849061730319545, 0.05835429374759529, 0.058198287579864526, 0.0581135590499201, 0.058116125451178446, 0.058053913601226005, 0.05778964254674809, 0.05758517102791553, 0.05755010707093653, 0.05770460313439223, 0.05781013573505308, 0.05800712838323759, 0.05798997375944708, 0.05783049892940805, 0.05791654960033941, 0.05808132070412489, 0.05834513273233873, 0.05847546943075319, 0.05853814054706032, 0.05898151043914846, 0.05926923234628218, 0.05915826393726877, 0.05897930785078761, 0.05882968629312901, 0.05888172412346193, 0.05922180854949722, 0.059543237566998294, 0.05956592061514546, 0.059498788281696816, 0.0593072279685038, 0.059129776071320236, 0.059176452994054926, 0.05917172647068018, 0.059021830158337237, 0.05906973418366674, 0.05922724356901222, 0.05949887805539689, 0.05965448628011618, 0.05973177197423492, 0.059785791110136245, 0.05979947770864753, 0.05961243383665556, 0.05956954494318309, 0.059631519226009184, 0.05955347523147965, 0.05957387356648812, 0.05984741466955572, 0.05996790481251774, 0.05998552910226325, 0.05999463309734478, 0.060033105363585866, 0.06029786780356588, 0.06034456419075758, 0.06017239876594633, 0.06042705374105978, 0.060803848403433565, 0.06092446282050737, 0.06084601982806343, 0.06070235727298172, 0.060610719978985074, 0.060502700889542876, 0.06077111466178934, 0.061272780406834176, 0.061598457345821075, 0.06164533842010082, 0.06153796868616372, 0.061681041815082695, 0.061519351759070105, 0.061539787841007394, 0.06188657260971695, 0.062130449935740545, 0.06251427418034602, 0.06268078979033131, 0.06259314095894787, 0.06285465630234868, 0.06300278091371077, 0.06292148510619504, 0.06291121650808629, 0.06301921360638654, 0.06290882750132862, 0.06260646620116307, 0.06254740751492863, 0.06253275111283725, 0.062185846937144955, 0.06176217546181761, 0.061595623072947775, 0.06161769152944554, 0.061515606997264924, 0.061230473418280196, 0.06108020738836829, 0.06111605673439989, 0.06143338161929832, 0.061943260532714796, 0.0623765923373375, 0.06241780215053852, 0.062282405076519484, 0.06285432803323897, 0.06333463152854575, 0.06344692179057417, 0.06339380231015702, 0.06340173700355675, 0.06328930719125364, 0.06318893182233565, 0.06342725863857418, 0.06349826209108346, 0.06348117820011252, 0.06360757412461618, 0.06389900537967479, 0.0638481323831278, 0.06372179236339058, 0.06358370520619722, 0.06340755093846154, 0.06325532126578519, 0.0633012533261907, 0.06345686904578351, 0.06334338549473613, 0.06311413881828522, 0.06329163510110436, 0.06360925328645695, 0.06366467873017276, 0.06365587043483395, 0.06382993275723649, 0.06398114289313687, 0.06401653234480403, 0.06431832871275917, 0.0648819575540269, 0.06528964162110579, 0.0655909471301273, 0.06618923692619691, 0.06625957912652161, 0.06589397905505291, 0.06552581761878384, 0.06533348760311737, 0.06539589984460288, 0.0654797452927127, 0.06576461488355631, 0.06572324487756398, 0.06544453624824169, 0.06542212435457805, 0.06562807978446561, 0.06557293885618427, 0.06530414637882531, 0.06524377237191567, 0.06541624859201278, 0.06521251812224975, 0.06477739813280019, 0.06430784858143557, 0.06395131372346644, 0.064131251643702, 0.06436340409170381, 0.06441068708584556, 0.06455487958159953, 0.06492517416553814, 0.06502826862079854, 0.06476978661412656, 0.064687409213245, 0.06456926848743173, 0.06473682079778774, 0.06466086240748442, 0.06432963675902499, 0.0643641878849901, 0.06445002780951595, 0.06439899859434708, 0.06443746070982749, 0.06459563635138867, 0.06455811142810938, 0.06437437333288537, 0.06430632164375004, 0.0645845722525132, 0.0650972608017537, 0.06540974531906363, 0.06531943338995706, 0.06525618371541234, 0.06532237594109518, 0.0652254951951787, 0.06543604519620493, 0.0654980534524559, 0.06545943576166603, 0.06569144811389263, 0.06603558473059541, 0.06655358921389694, 0.06679552341103104, 0.06676183322343612, 0.06659215769042341, 0.06662641250346234, 0.0668956105450063, 0.0672483075581444, 0.06733342754121424, 0.0674076739278912, 0.06760397058201066, 0.0677399576868238, 0.06747021027877133, 0.06695508575338326, 0.06655651423458005, 0.0662372636629174, 0.0659154247181176, 0.06554498465318495, 0.06504607922278113, 0.06486384042803868, 0.06455069726507413, 0.06412656290080102, 0.06379165118981858, 0.06359780668294697, 0.06355654474336683, 0.06401629456036927, 0.06453026866287834, 0.0645838605766406, 0.06491064775724087, 0.0653274576946064, 0.06558843510754787, 0.06558422244547026, 0.0655167488078935, 0.0656473145227912, 0.06550341239805146, 0.06548774388866171, 0.06551045101803897, 0.06541924741276649, 0.06551240084794706, 0.06534918301198812, 0.0651178124235769, 0.06488918480121025, 0.06470051408133454, 0.06511223326375852, 0.0653989258655387, 0.06543459984582736, 0.06564697038114765, 0.06617586671491983, 0.06665170819721769, 0.06691547908436127, 0.06703781797697252, 0.06704057412845861, 0.06745879209874991, 0.06810090895106663, 0.06853024445482657, 0.06848119418517787, 0.06813674096197864, 0.06813856447314032, 0.06825613684843382, 0.06819525166405714, 0.06860283311081988, 0.06867263526999105, 0.06858311465146126, 0.06892044625487682, 0.06898648402761841, 0.06930900934640047, 0.06944233264341036, 0.06981461434447374, 0.06979472958815176, 0.06938995441610407, 0.06978933104182716, 0.07007756352117503, 0.07049565383744578, 0.07083489285000842, 0.07080227542352063, 0.07090503939383314, 0.07091154272287309, 0.07121962624044907, 0.07149976396283779, 0.07164639037395287, 0.0713804020949169, 0.0708607446549932, 0.07003182484851474, 0.06904321781385124, 0.06865830378640851, 0.06881504879697581, 0.0688203741847008, 0.06878945033322129, 0.06880164776929654, 0.06880870926290106, 0.06875119465455616, 0.06898059211722958, 0.06951175092191238, 0.06977785729551486, 0.07010103434407938, 0.07052556416361756, 0.07062904258858685, 0.07068716818496748, 0.07074781274025763, 0.07093951403321372, 0.07072233962024523, 0.07062042303277236, 0.070601633529755, 0.07099319202226002, 0.071651847821549, 0.0717798446817815, 0.07201350096771532, 0.07221085809415598, 0.07227260378362041, 0.07182016201800895, 0.07152142543610011, 0.07177922995606084, 0.07207041964366033, 0.0721323135138434, 0.07166885889162893, 0.07107900322435981, 0.07134058727014606, 0.07149188140026007, 0.07164785167583101, 0.07191976720280611, 0.0717393297866443, 0.07176114642820759, 0.07152657403566205, 0.07143463144163159, 0.0715947526380863, 0.07151796832286146, 0.07108664504294693, 0.07149846720039219, 0.07195647366547007, 0.07210011243542007, 0.07245708329461965, 0.07301811907872542, 0.0733474980931388, 0.0733298941987957, 0.07326893585088692, 0.07366465102727653, 0.07437688377562172, 0.07471325583909132, 0.07548709023701852, 0.07567299866009454, 0.07516057237441745, 0.07531985960292487, 0.07576682992739789, 0.0760622568462721, 0.07621698935003257, 0.07611992301063919, 0.07576384069922511, 0.07571969917405591, 0.07555902658130545, 0.07521804633051014, 0.07492186996823948, 0.07456139734381151, 0.07456333740170325, 0.07491030902393096, 0.07482632028586995, 0.07425197160232878, 0.07359598726747656, 0.07304085017682364, 0.07329374724030963, 0.07361538918127045, 0.07378962920183557, 0.0739832416339514, 0.07408863276241512, 0.07476972138050456, 0.0754187577137337, 0.0751396779628803, 0.07474425995140388, 0.07531054931899812, 0.07598417779014986, 0.07677522988735498, 0.07674673177497322, 0.07592432124085823, 0.0757396177126806, 0.07567968417056369, 0.07534770118605161, 0.07496737908999981, 0.0748987568103376, 0.07518782756769954, 0.07512304653559833, 0.07527818297402018, 0.07551433409785364, 0.07577043912682407, 0.07592096198522144, 0.07586679563082117, 0.07576141868699234, 0.07513366796312192, 0.07441456376279197, 0.07421729701592196, 0.07464758603651049, 0.07511029714565555, 0.07553238214082293, 0.07591152140032151, 0.07559916632474904, 0.07489224460363565, 0.07449170229809718, 0.07441365368932239, 0.07474254608785505, 0.07472772018057286, 0.07458210400858979, 0.07443155912787784, 0.0744136180881394, 0.07449042161992336, 0.07420588830268028, 0.0740131333001016, 0.07436828966896047, 0.0749843618062533, 0.07487691474288538, 0.07453029885176941, 0.07480969076645456, 0.07494821069636691, 0.07451997153859874, 0.07391212033578697, 0.07356077504445596, 0.0732426032889345, 0.07310227053599341, 0.07340778100682183, 0.07312939592277856, 0.07306009158842137, 0.07318175365923536, 0.07298962507972494, 0.07291894139845799, 0.07262342383245113, 0.07262930080181462, 0.07265450991053982, 0.07236691276099988, 0.07214135708214728, 0.0717763760991981, 0.0713799898047553, 0.07138670369333747, 0.07176957901277647, 0.07179632048460122, 0.0714183451193027, 0.07105802678707647, 0.07102995492304462, 0.07096638162134554, 0.07139033156594829, 0.07207134501568317, 0.07214758323887913, 0.0719831321539437, 0.07188550067788597, 0.07152597946285186, 0.07086266855184008, 0.0705237249020463, 0.07076690616052914, 0.07093671660493885, 0.07112610478696334, 0.07089222916306472, 0.07034210915344088, 0.06987139094021715, 0.0697434158244755, 0.0699049140983836, 0.06991873396047696, 0.0698126657844245, 0.06993300100333036, 0.07005362606947467, 0.07029306309731605, 0.07074091517669348, 0.07055688411213651, 0.06995100635868544, 0.06999990045962066, 0.07027607257771767, 0.07032970380644783, 0.07041718085466127, 0.0708162194831953, 0.07096815110236425, 0.07034043384700803, 0.07011671257193813, 0.07001979417551832, 0.06936670825175828, 0.06921184636109246, 0.06997196931258073, 0.07055806015648178, 0.07050930910232103, 0.07021581707007131, 0.07023102389679588, 0.0705171778717607, 0.07034135461899055, 0.07032919361873465, 0.0702222884045411, 0.0700041396296305, 0.06966699075065196, 0.06946694041990745, 0.0692535111027861, 0.06892174411580713, 0.06899178687377526, 0.06912089061671028, 0.06903569645824711, 0.06899435642200655, 0.06898393559025631, 0.06925177041563821, 0.06928819798798488, 0.06950638317614868, 0.06990686658284949, 0.069937613639409, 0.06969172741336353, 0.0695978086563733, 0.069645722890744, 0.069608520525914, 0.06994310970670699, 0.0701252529248026, 0.07009047621698676, 0.07015372413938484, 0.07007222690055293, 0.06980089581053216, 0.06976638592353271, 0.06989433500195792, 0.06984056178754378, 0.07018951000067535, 0.07026207315023386, 0.07013653160652362, 0.06971337471999363, 0.06949506094795896, 0.06954042483931445, 0.06988818146650361, 0.0700593407605829, 0.06988463150595385, 0.06981942433934754, 0.06960273581477495, 0.06941850081580722, 0.06950360697352917, 0.06954483226947913, 0.06908291758843788, 0.06898964844059717, 0.06925958829962002, 0.06944148437039036, 0.06922762099700727, 0.0690226936455336, 0.06871840351218725, 0.06822575460796536, 0.0681517674488937, 0.06791438748879494, 0.06762006442203652, 0.06716215046709095, 0.06685974641292941, 0.0670697446341426, 0.06692005168300318, 0.06638062638844949, 0.06619481710286454, 0.06641738521625891, 0.06649151340554021, 0.06668506499868586, 0.06700107582130961, 0.06703444939056896, 0.06670528004647863, 0.06669011545863313, 0.06660984393011302, 0.06635992581542573, 0.06635986939837704, 0.06630641698142743, 0.06587771441598791, 0.06586062452597795, 0.0660445829832135, 0.06568976837831772, 0.0650998187572852, 0.06508025353686153, 0.06535039761942872, 0.0653329576966507, 0.06562783564151954, 0.06597265488084833, 0.06595730244246617, 0.06591950577011224, 0.06626596266194287, 0.06653291849260302, 0.0667312569317782, 0.06679312216244736, 0.06663765411806148, 0.06671776234972532, 0.0672116477680752, 0.06789843462279924, 0.06819416232017529, 0.06817833841707986, 0.06813278662165018, 0.06795443948643864, 0.06776364088128152, 0.06752333862307854, 0.06746698378744045, 0.06788482093811538, 0.06813516248503809, 0.06809945665465594, 0.06809451781199083, 0.06823189714052748, 0.06835080131702335, 0.06818459787345878, 0.06798933568509886, 0.06801005400087398, 0.06819419064950595, 0.06811452953872814, 0.06815187840510226, 0.06840882647786779, 0.06832639087011157, 0.06830526348658941, 0.06864233380600068, 0.0690551602961352, 0.06917916984603371, 0.06921218755424581, 0.06969563837158824, 0.06996149194509994, 0.07026668046086647, 0.07069367883133512, 0.07076546576316423, 0.07053544694872828, 0.0703792144670839, 0.07030748150290875, 0.07013387523522431, 0.0698462814259397, 0.06993069008323106, 0.07020020702205836, 0.07023612705847129, 0.07006407498579463, 0.06962262608139082, 0.06908783689469519, 0.0686973050992647, 0.0683555716007936, 0.06780316964023911, 0.06772072668156436, 0.06791257975519924, 0.067640949984472, 0.06737531617282408, 0.06745637192519487, 0.06730529968807444, 0.0669224520635189, 0.06691940152487728, 0.06651629965464607, 0.06626866957317193, 0.06672646409802181, 0.06679368206222222, 0.06647808698952684, 0.06616201151844286, 0.06606067976595174, 0.0660675841184347, 0.0662128076281085, 0.06624147287949793, 0.06610736872417719, 0.06630147702093676, 0.06665997202347748, 0.06698239630962459, 0.06701116591272085, 0.06701971208204424, 0.06731992062212395, 0.06741019568219614, 0.06729298540083409, 0.06713508270824836, 0.06717465399372241, 0.06750618626550423, 0.06771350623134297, 0.06769401315269749, 0.06800231078209155, 0.06800024705463349, 0.0678024469235352, 0.06786704872548519, 0.06780051525739515, 0.06769987601247199, 0.0677033665979151, 0.06777338534207916, 0.06745657071579944, 0.0668863910345141, 0.06664948275598286, 0.06658387012241518, 0.06654583798668545, 0.0665243049115165, 0.06665454890388534, 0.06663079424542968, 0.06627333646959721, 0.06634314765219142, 0.06667827357523215, 0.06692103246518451, 0.06692272773761974, 0.06691964364152582, 0.0672487901053529, 0.06754687803659505, 0.0675133462284553, 0.06728014031578415, 0.0672865085493566, 0.06728039475703668, 0.06719826284517244, 0.06714765794129496, 0.06687013286068692, 0.06690624650758843, 0.06690782130198111, 0.0670214195921457, 0.06698253283669911, 0.06647996621120988, 0.0662575460046363, 0.06620786590602312, 0.0658760083682337, 0.06569779070146063, 0.06581590126043674, 0.0660121375854278, 0.0658839994494414, 0.06537501950296701, 0.06508547327998757, 0.06509115662454734, 0.06522329562241228, 0.06532797004190488, 0.0652629084097425, 0.06519548937421499, 0.06537329348296803, 0.06584306065522932, 0.06635096220290122, 0.06695229507635128, 0.06734342698678739, 0.06754335725719865, 0.06751128543334416, 0.06714977144810834, 0.06698703681512738, 0.06717175335418371, 0.0673591194754693, 0.06726414545554076, 0.06696570982487855, 0.0668490397012521, 0.06670153284773173, 0.06691740793880778, 0.06731864469003128, 0.06752792180739703, 0.06764131245989975, 0.06763529485560843, 0.06762040789426703, 0.06764963037215961, 0.06800978684013476, 0.06822980459071971, 0.06836698919346415, 0.06821093891743715, 0.06849107773562413, 0.0690991845108852, 0.06910553552458616, 0.06895822533637733, 0.0689340434375409, 0.06912872059485826, 0.06929723534459156, 0.06933195171403381, 0.06969589902711557, 0.07007098350231587, 0.06969620427564692, 0.06940682708339295, 0.06937756078586868, 0.06910439200640872, 0.06912142871109662, 0.06878595005242519, 0.06861830913357539, 0.06858035263636543, 0.06837729010662309, 0.06878783131470524, 0.06885286358238629, 0.06900061354769986, 0.06942415901220644, 0.06966331231349926, 0.0699751809618948, 0.06998004368445485, 0.06964756780098813, 0.06980466421948715, 0.07019083051732833, 0.07032392435783268, 0.07034927047027952, 0.07036942462801582, 0.07064116100955971, 0.07089692646954095, 0.07105882147560698, 0.07111252285759727, 0.07095295704658654, 0.07056648856215247, 0.07029341058377087, 0.07017187806794659, 0.07017968230935714, 0.07013471349698344, 0.06945011983809102, 0.06922738962236392, 0.06915041647482624, 0.06881217862841925, 0.06894881785074733, 0.06907965930665276, 0.06908709285430395, 0.06925001075656863, 0.06931492794542138, 0.06884094892644432, 0.0681400929105796, 0.06777301344319477, 0.06761132605982685, 0.06739573953208773, 0.06727569412903925, 0.067390835728369, 0.06755745991825467, 0.06762596407812606, 0.0677021705202099, 0.06763991006260528, 0.06736068868082598, 0.06747993161227901, 0.06771901500497049, 0.06781404551776624, 0.06792231826053356, 0.06798833659023867, 0.06795033308594926, 0.06771978631817731, 0.06748410987753127, 0.06746074864194071, 0.06743619919083628, 0.06742576886707444, 0.06757204344854281, 0.06770649726118394, 0.06726701088155038, 0.06673809610265133, 0.06667216140011274, 0.06677678127217998, 0.06678262765547484, 0.06708056416008008, 0.06708975076399537, 0.06684512220955023, 0.06722236570358517, 0.06768824980358917, 0.06752007726627135, 0.06720301192889926, 0.06733557283830231, 0.0675482954438379, 0.06761499506834827, 0.06729972822402272, 0.06720058053636103, 0.06703398752841144, 0.06701026161738437, 0.06732136731571203, 0.06791258830088612, 0.06834346509334448, 0.06834069114463316, 0.0678592890701066, 0.06707290046143459, 0.06665896678657963, 0.06674814810494926, 0.06684190667189276, 0.06691970846341677, 0.06675412924658235, 0.06652737066299783, 0.06657448642645969, 0.06661993498824972, 0.0662482995041999, 0.06605130022970157, 0.06642672328353064, 0.06658364661729847, 0.06619722949677824, 0.06575994184380453, 0.06566318739409029, 0.06537284388697714, 0.06516397722029249, 0.06516392139045729, 0.0649162017127614, 0.0648552837361134, 0.06488292874543296, 0.06478192603685556, 0.06454673436617708, 0.06420482557757785, 0.0643010023674997, 0.06449065125050006, 0.06461437550651956, 0.06491071641840854, 0.06501901562923859, 0.06512549013308595, 0.06516516882108792, 0.06509058246126873, 0.065456347524411, 0.06583905848821102, 0.06592676106240059, 0.06593665103097436, 0.06604888466930474, 0.06578982663099293, 0.06559973570220441, 0.06565144317183025, 0.06561285172579155, 0.06543215940376723, 0.06557485335725585, 0.06591935207920832, 0.06553760009758872, 0.0652198001615331, 0.06555188312439424, 0.06590700794083998, 0.06594947464926063, 0.0660313492425374, 0.06584176776558183, 0.06564267920985481, 0.06597058316628933, 0.06609258767837249, 0.06585688675592252, 0.0656489888396708, 0.06570413613424524, 0.06592053487629126, 0.0660151817992942, 0.06611297124125176, 0.06616270971620683, 0.06613771929626347, 0.0659705597338008, 0.06597803272526165, 0.06613589486849548, 0.06596667733956835, 0.06594170578504616, 0.06597539026594842, 0.06592020080278471, 0.06581321218429347, 0.06545001649539332]\n",
            "Reacher here\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import torch as th\n",
        "import torch.nn.functional as f\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "th.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Construct the environment\n",
        "## We have three different environment configurations:\n",
        "\n",
        "class IlluToyMDP():\n",
        "\n",
        "  def __init__(self, adversary_policy=None, adversary_policy1=None, victim_policy=None, victim_policy1 = None, mode=\"unattacked\"):\n",
        "    # 3 different modes: unattacked, attacked_adversary, attacked_victim\n",
        "    self.step_ctr = 0\n",
        "    self.set_mode(mode, adversary_policy, adversary_policy1, victim_policy, victim_policy1)\n",
        "    self.reward_matrix = th.FloatTensor([[-1, 1],[1, 0.5]])  ## state x action\n",
        "    self.prob_s0 = th.tensor([1/3., 2/3.], dtype=th.float)\n",
        "    self.trans_prob = f.softmax(th.rand(2, 2, 2), dim=-1)\n",
        "    pass\n",
        "\n",
        "  def set_mode(self, mode, adversary_policy=None, adversary_policy1 = None, victim_policy=None, victim_policy1 = None):\n",
        "    if mode == \"attacked_adversary\":\n",
        "      assert victim_policy is not None, \"Need a valid victim policy!\"\n",
        "    elif mode == \"attacked_victim\":\n",
        "      assert adversary_policy is not None, \"Need a valid adversary policy!\"\n",
        "    self.mode = mode\n",
        "    self.adversary_policy = adversary_policy\n",
        "    self.victim_policy = victim_policy\n",
        "    self.adversary_policy1 = adversary_policy1\n",
        "    self.victim_policy1 = victim_policy1\n",
        "\n",
        "  def step(self, action, victim_a0=None, action1=None):\n",
        "    info = {}\n",
        "    if self.step_ctr == 0:\n",
        "      if self.mode == \"unattacked\":\n",
        "        reward = self.reward_matrix[self.state, action].item()\n",
        "        self.state = th.multinomial(self.trans_prob[self.state, action_v], 1)\n",
        "      elif self.mode == \"attacked_adversary\":\n",
        "        action_v = th.multinomial(self.victim_policy(action), 1)\n",
        "        reward = -self.reward_matrix[self.state, action_v].item()\n",
        "        info[\"victim_action\"] = action_v\n",
        "        self.state = th.multinomial(self.trans_prob[self.state, action_v], 1)\n",
        "        self.obs = self.state\n",
        "      elif self.mode == \"attacked_victim\":\n",
        "        reward = self.reward_matrix[self.state, action].item()\n",
        "        self.obs = self.adversary_policy(self.state)\n",
        "        self.state = th.multinomial(self.trans_prob[self.state, action_v], 1)\n",
        "      obs = self.obs\n",
        "      done = False\n",
        "      terminated = False\n",
        "      return obs, reward, done, terminated, info\n",
        "    elif self.step_ctr == 1:\n",
        "      if self.mode == \"unattacked\":\n",
        "        reward = self.reward_matrix[self.state, action].item()\n",
        "      elif self.mode == \"attacked_adversary\":\n",
        "        action_v1 = th.multinomial(self.victim_policy1(action, action1, victim_a0), 1) # add additional terms (ao ans Oo)\n",
        "        reward = -self.reward_matrix[self.state, action_v1].item()\n",
        "        info[\"victim_action\"] = action_v1\n",
        "        #self.state = th.multinomial(self.trans_prob[self.state, action_v1], 1) # check this\n",
        "        self.obs = self.state# check this\n",
        "      elif self.mode == \"attacked_victim\":\n",
        "        reward = self.reward_matrix[self.state, action].item()\n",
        "      obs = self.obs\n",
        "      done = True\n",
        "      terminated = False\n",
        "      self.step_ctr = 2\n",
        "      return obs, reward, done, terminated, info\n",
        "\n",
        "  def reset(self):\n",
        "    self.step_ctr = 0\n",
        "    self.state = th.multinomial(self.prob_s0, 1)\n",
        "    if self.mode == \"unattacked\":\n",
        "      self.obs = self.state\n",
        "    elif self.mode == \"attacked_adversary\":\n",
        "      self.obs = self.state\n",
        "    elif self.mode == \"attacked_victim\":\n",
        "      self.obs = self.adversary_policy(self.state) # step = 0 was here\n",
        "    return self.obs\n",
        "\n",
        "  def exact_kl(self,\n",
        "               order,\n",
        "               adversary_policy,\n",
        "               adversary_policy1,\n",
        "               victim_policy,\n",
        "               victim_policy1):\n",
        "    assert order in [\"attacked|unattacked\", \"unattacked|attacked\"]\n",
        "    action_space = list(range(self.reward_matrix.shape[1]))\n",
        "    obs_space = list(range(self.reward_matrix.shape[0]))\n",
        "    state_space = obs_space\n",
        "    exact_kls = []\n",
        "\n",
        "    if True:\n",
        "      for o in obs_space:\n",
        "        o = th.tensor(o)\n",
        "        for a in action_space:\n",
        "          a = th.tensor(a)\n",
        "          for o1 in obs_space:\n",
        "            o1 = th.tensor(o1)\n",
        "            for a1 in action_space:\n",
        "              a1 = th.tensor(a1)\n",
        "              P_a = 0\n",
        "             # calculate P_a(tau_o) = P_a(o, a) = int_s pi_a(o|s)p(s)pi_v(a|o) *\n",
        "              for s in state_space:\n",
        "                s = th.tensor(s)\n",
        "                for s1 in state_space:\n",
        "                  s1 = th.tensor(s1)\n",
        "                  P_a += self.prob_s0[s] * adversary_policy(s)[o] * victim_policy(o)[a] * self.trans_prob[s, a][s1] * adversary_policy1(s1, o, a)[o1] * victim_policy1(o, o1, a)[a1]\n",
        "              P = self.prob_s0[o] * victim_policy(o)[a] * self.trans_prob[o, a][o1] * victim_policy1(o, o1, a)[a1]\n",
        "              if order == \"attacked|unattacked\":\n",
        "                if not(P_a.item() == 0.0 == P.item()):\n",
        "                  contrib = P_a.clone() * (th.log(P_a.clone()) - th.log(P.clone()))\n",
        "                  exact_kls.append(contrib)\n",
        "              else:\n",
        "                if not(P_a.item() == 0.0 == P.item()):\n",
        "                  contrib = P.clone() * (th.log(P.clone()) - th.log(P_a.clone()))\n",
        "                  exact_kls.append(contrib)\n",
        "    mean_kl = th.mean(th.cat([ek.reshape(1) for ek in exact_kls]))\n",
        "    if th.isnan(mean_kl).item() or mean_kl.item() < 0.0:\n",
        "        assert False, \"FATAL\"\n",
        "    return mean_kl\n",
        "\n",
        "\n",
        "def victim_policy_func(obs, obs1, a):\n",
        "  if obs.item() == 0 and a.item() == 0 and obs1 == 0:\n",
        "    return th.tensor([0.0, 1.0], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 0 and obs1 == 1:\n",
        "    return th.tensor([1.0, 0.0], dtype=th.float)\n",
        "  elif obs.item() == 1 and a.item() == 0 and obs1 == 0:\n",
        "    return th.tensor([0.0, 1.0], dtype=th.float)\n",
        "  elif obs.item() == 1 and a.item() == 0 and obs1 == 1:\n",
        "    return th.tensor([1.0, 0.0], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 1 and obs1 == 0:\n",
        "    return th.tensor([0.0, 1.0], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 1 and obs1 == 1:\n",
        "    return th.tensor([0.0, 1.0], dtype=th.float)\n",
        "  else:\n",
        "    return th.tensor([0.0, 1.0], dtype=th.float)\n",
        "\n",
        "\n",
        "def victim_policy_func1(obs, obs1, a):\n",
        "  if obs.item() == 0 and a.item() == 0 and obs1 == 0:\n",
        "    return th.tensor([0.25, 75], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 0 and obs1 == 1:\n",
        "    return th.tensor([0.65, 0.35], dtype=th.float)\n",
        "  elif obs.item() == 1 and a.item() == 0 and obs1 == 0:\n",
        "    return th.tensor([0.35, 0.65], dtype=th.float)\n",
        "  elif obs.item() == 1 and a.item() == 0 and obs1 == 1:\n",
        "    return th.tensor([0.2, 0.8], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 1 and obs1 == 0:\n",
        "    return th.tensor([0.35, 0.65], dtype=th.float)\n",
        "  elif obs.item() == 0 and a.item() == 1 and obs1 == 1:\n",
        "    return th.tensor([0.2, 0.8], dtype=th.float)\n",
        "  else:\n",
        "    return th.tensor([0.25, 0.75], dtype=th.float)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exact_dist():\n",
        "    # Find optimal adversary policy with REINFORCE (with exact distributional constraints)\n",
        "    env = IlluToyMDP()\n",
        "    n_states = 2\n",
        "    n_actions = 2\n",
        "    params = th.nn.Parameter(th.zeros(n_states, n_states).uniform_().double(), requires_grad=True)\n",
        "    params1 = th.nn.Parameter(th.zeros(n_states, n_states, n_actions, n_states).uniform_().double(), requires_grad=True)\n",
        "    n_rollout_steps = 100\n",
        "    n_rollout_steps_eval = 1000\n",
        "    n_updates = 10\n",
        "    n_episodes = 1000\n",
        "    alpha = 5*10E-3\n",
        "    alpha_lambda = 10.0\n",
        "    lmbda = 1.0  # arbitrary!\n",
        "    adversary_policy = lambda s0: f.softmax(params[s0])\n",
        "    adversary_policy1 = lambda s1, o0, a0: f.softmax(params1[s1, o0, a0])\n",
        "    #victim_policy = lambda obs: th.tensor([0.5, 0.5], dtype=th.float) if obs.item() == 0 else th.tensor([0.4, 0.6], dtype=th.float)\n",
        "    victim_policy1 = lambda o0, o1, a0: victim_policy_func(o0, o1, a0)\n",
        "    victim_policy = lambda o0:th.tensor([0.0, 1.0], dtype=th.float) if o0.item() == 0 else th.tensor([1.0, 0.0], dtype=th.float)\n",
        "    #victim_policy1 = lambda o0, o1, a0:th.tensor([0.0, 1.0], dtype=th.float) if o1.item() == 0 else th.tensor([1.0, 0.0], dtype=th.float)\n",
        "\n",
        "    env.set_mode(\"attacked_adversary\", adversary_policy = adversary_policy, adversary_policy1 = adversary_policy1, victim_policy=victim_policy, victim_policy1 = victim_policy1)\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    trajectories = [defaultdict(lambda: {})]\n",
        "    trajectories[-1][\"obs\"] = obs.item()\n",
        "    mean_reward_list = []\n",
        "    loss_list = []\n",
        "    kl_list = []\n",
        "    for i in range(n_episodes):\n",
        "        trajectories[-1][\"obs\"] = obs.item()\n",
        "        # rollout phase\n",
        "        for j in range(n_rollout_steps):\n",
        "            action = th.multinomial(adversary_policy(obs), 1)\n",
        "            obs1, reward, done, _, info = env.step(action)\n",
        "            trajectories[-1][\"reward\"] = reward\n",
        "            trajectories[-1][\"action\"] = action.item()\n",
        "            trajectories[-1][\"obs1\"] = obs1.item()\n",
        "            victim_action = info[\"victim_action\"]\n",
        "            trajectories[-1][\"victim_action\"] = victim_action\n",
        "            action1 = th.multinomial(adversary_policy1(obs1.item(), action.item(), victim_action.item()), 1)\n",
        "            env.step_ctr += 1\n",
        "            _, reward1, done, _, info = env.step(action, victim_action, action1)\n",
        "            trajectories[-1][\"action1\"] = action1.item()\n",
        "            trajectories[-1][\"reward1\"] = reward1\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                trajectories.append(defaultdict(lambda: {}))\n",
        "                trajectories[-1][\"obs\"] = obs.item()\n",
        "\n",
        "\n",
        "        # train phase\n",
        "\n",
        "        kl_exact_lst = []\n",
        "        for j in range(n_updates):\n",
        "            kl_exact = env.exact_kl(\"attacked|unattacked\",\n",
        "                                    adversary_policy = adversary_policy,\n",
        "                                    adversary_policy1 = adversary_policy1,\n",
        "                                    victim_policy=victim_policy,\n",
        "                                    victim_policy1 = victim_policy1)\n",
        "            loss = 0\n",
        "            for n, traj in enumerate(trajectories[:-1]):\n",
        "                try:\n",
        "                    prob1 = f.softmax(params[traj[\"obs\"]])[traj[\"action\"]]\n",
        "                    prob2 = th.squeeze(f.softmax(params1[traj[\"obs1\"], traj[\"action\"], traj[\"victim_action\"]]))[traj[\"action1\"]]\n",
        "                    loss += ( (th.log(prob1) * traj[\"reward\"]) + (th.log(prob2) * traj[\"reward1\"]) ) # - lmbda * kl_exact\n",
        "                    kl_exact_lst.append(kl_exact.detach().cpu().numpy())\n",
        "                except:\n",
        "                    #print(\"WARNING: loss addition failed\")\n",
        "                    pass\n",
        "\n",
        "            loss /= len(trajectories)\n",
        "            loss.backward()\n",
        "            params.data += alpha * params.grad.data\n",
        "            params.grad.zero_()\n",
        "\n",
        "            params1.data += alpha * params1.grad.data\n",
        "            params1.grad.zero_()\n",
        "\n",
        "            # Now we do the dual ascent step\n",
        "            with th.no_grad():\n",
        "                kl_exact = env.exact_kl(\"attacked|unattacked\",\n",
        "                                        adversary_policy=adversary_policy,\n",
        "                                        adversary_policy1 = adversary_policy1,\n",
        "                                        victim_policy=victim_policy,\n",
        "                                        victim_policy1=victim_policy1)\n",
        "\n",
        "            lmbda = max(lmbda + alpha_lambda * kl_exact, 0)\n",
        "\n",
        "        print(\"LOSS:\", loss.item())\n",
        "        loss_list.append(loss.item())\n",
        "        print(f\"params: {f.softmax(params, -1)}\")\n",
        "        print(f\"params1: {f.softmax(params1, -1)}\")\n",
        "        print(\"lambda: \", lmbda.item())\n",
        "        print(\"kl: \", np.mean(kl_exact_lst))\n",
        "        kl_list.append(np.mean(kl_exact_lst))\n",
        "\n",
        "        # evaluate\n",
        "        reward_lst = []\n",
        "        obs = env.reset()\n",
        "\n",
        "        for j in range(n_rollout_steps_eval):\n",
        "            action = th.multinomial(adversary_policy(obs), 1)\n",
        "            obs1, reward, done, _, info = env.step(action)\n",
        "            victim_action = info[\"victim_action\"]\n",
        "            action1 = th.multinomial(adversary_policy1(obs1.item(), action.item(), victim_action.item()), 1)\n",
        "            env.step_ctr += 1\n",
        "            _, reward1, done, _, info = env.step(action, victim_action, action1)\n",
        "            tot_reward = reward + reward1\n",
        "            reward_lst.append(tot_reward)\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "\n",
        "        print(\"TEST REWARD MEAN: \", np.mean(reward_lst))\n",
        "        mean_reward_list.append(np.mean(reward_lst))\n",
        "\n",
        "        trajectories = [defaultdict(lambda: [])]\n",
        "\n",
        "    print(kl_list)\n",
        "    print(\"Reacher here\")\n",
        "    plt.plot(mean_reward_list, color=\"green\")\n",
        "    plt.title(\"Average Reward Per Episode\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.xlabel(\"Num Episodes\")\n",
        "    plt.show()\n",
        "    plt.plot(kl_list, color=\"blue\")\n",
        "    plt.title(\"Average KL Per Episode\")\n",
        "    plt.ylabel(\"Average KL\")\n",
        "    plt.xlabel(\"Num Episodes\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\texact_dist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}